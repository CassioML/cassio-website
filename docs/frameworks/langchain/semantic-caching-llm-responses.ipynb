{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9288bca1",
   "metadata": {},
   "source": [
    "# Semantic LLM caching\n",
    "\n",
    "Save on tokens and latency with a LLM response cache based on semantic similarity (as opposed to exact match), powered by Vector Search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152c180",
   "metadata": {},
   "source": [
    "_**NOTE:** this uses Cassandra's \"Vector Search\" capability.\n",
    "Make sure you are connecting to a vector-enabled database for this demo._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8771872b",
   "metadata": {},
   "source": [
    "The Cassandra-backed \"semantic cache\" for prompt responses is imported like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8ce64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import CassandraSemanticCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7bc8f",
   "metadata": {},
   "source": [
    "As usual, a database connection is needed to access Cassandra. The following assumes\n",
    "that a _vector-search-capable Astra DB instance_ is available. Adjust as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb1a050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cqlsession import getCQLSession, getCQLKeyspace\n",
    "cqlMode = 'astra_db' # 'astra_db'/'local'\n",
    "session = getCQLSession(mode=cqlMode)\n",
    "keyspace = getCQLKeyspace(mode=cqlMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9f6d2",
   "metadata": {},
   "source": [
    "An embedding function and an LLM are needed.\n",
    "\n",
    "Below is the logic to instantiate the LLM and embeddings of choice. We chose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a946b81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM+embeddings from OpenAI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llm_choice import suggestLLMProvider\n",
    "\n",
    "llmProvider = suggestLLMProvider()\n",
    "# (Alternatively set llmProvider to 'GCP_VertexAI', 'OpenAI', 'Azure_OpenAI' ... manually if you have credentials)\n",
    "\n",
    "if llmProvider == 'GCP_VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    from langchain.embeddings import VertexAIEmbeddings\n",
    "    llm = VertexAI()\n",
    "    myEmbedding = VertexAIEmbeddings()\n",
    "    print('LLM+embeddings from Vertex AI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    os.environ['OPENAI_API_TYPE'] = 'open_ai'\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = OpenAI(temperature=0)\n",
    "    myEmbedding = OpenAIEmbeddings()\n",
    "    print('LLM+embeddings from OpenAI')\n",
    "elif llmProvider == 'Azure_OpenAI':\n",
    "    os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "    os.environ['OPENAI_API_VERSION'] = os.environ['AZURE_OPENAI_API_VERSION']\n",
    "    os.environ['OPENAI_API_BASE'] = os.environ['AZURE_OPENAI_API_BASE']\n",
    "    os.environ['OPENAI_API_KEY'] = os.environ['AZURE_OPENAI_API_KEY']\n",
    "    from langchain.llms import AzureOpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = AzureOpenAI(temperature=0, model_name=os.environ['AZURE_OPENAI_LLM_MODEL'],\n",
    "                      engine=os.environ['AZURE_OPENAI_LLM_DEPLOYMENT'])\n",
    "    myEmbedding = OpenAIEmbeddings(model=os.environ['AZURE_OPENAI_EMBEDDINGS_MODEL'],\n",
    "                                   deployment=os.environ['AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT'])\n",
    "    print('LLM+embeddings from Azure OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a5ae3",
   "metadata": {},
   "source": [
    "## Create the cache\n",
    "\n",
    "At this point you can instantiate the semantic cache.\n",
    "\n",
    "_Note: in the following it is made clear, through the way the `table` parameter is constructed, that different embeddings will require separate tables. This is done here to avoid mismatches when running this demo over and over with varying embedding functions: in most applications, where a single choice of embedding is made, there's no need to be this finicky and you cal usually leave the table name to its default value._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f04489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache = CassandraSemanticCache(\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    embedding=myEmbedding,\n",
    "    table_name=f'semantic_cache_{llmProvider}',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c6159",
   "metadata": {},
   "source": [
    "Make sure the cache starts empty with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc165a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8fe19e",
   "metadata": {},
   "source": [
    "Configure the cache at a LangChain global level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05ac5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.llm_cache = cassSemanticCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed086e91",
   "metadata": {},
   "source": [
    "## Use the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca683a93",
   "metadata": {},
   "source": [
    "Now try submitting a few prompts to the LLM and pay attention to the response times.\n",
    "\n",
    "If the LLM is actually run, they should be the order of a few seconds; but in case of a cache hit, it will be way less than a second.\n",
    "\n",
    "Notice that you get a cache hit even after rephrasing the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c051cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.3 ms, sys: 4.96 ms, total: 41.2 ms\n",
      "Wall time: 3.66 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes, although some have fewer or more.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_1 = \"How many eyes do spiders have?\"\n",
    "# A new question should take long\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71181e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.26 ms, sys: 3.16 ms, total: 11.4 ms\n",
      "Wall time: 403 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes, although some have fewer or more.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Second time, very same question, this should be quick\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc6d95b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.5 ms, sys: 770 µs, total: 20.2 ms\n",
      "Wall time: 1e+03 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes, although some have fewer or more.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_2 = \"How many eyes does a spider generally have?\"\n",
    "# Just a rephrasing: but it's the same question,\n",
    "# so it will just take the time to evaluate embeddings\n",
    "llm(SPIDER_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eedbd72",
   "metadata": {},
   "source": [
    "Time for a really new question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fcdae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.7 ms, sys: 3.96 ms, total: 35.7 ms\n",
      "Wall time: 3.33 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nNo, absence of proof is not the same as proof of absence. Absence of proof means that there is no evidence to support a claim, while proof of absence means that there is evidence to support the claim that something does not exist.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LOGIC_QUESTION_FORM_1 = \"Is absence of proof the same as proof of absence?\"\n",
    "# A totally new question\n",
    "llm(LOGIC_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a582be",
   "metadata": {},
   "source": [
    "Going back to the same question as earlier (not literally, though):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1951f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.1 ms, sys: 894 µs, total: 21 ms\n",
      "Wall time: 862 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes, although some have fewer or more.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_3 = \"How many eyes are on the head of a typical spider?\"\n",
    "# Trying to catch the cache off-guard :)\n",
    "llm(SPIDER_QUESTION_FORM_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020001df",
   "metadata": {},
   "source": [
    "And again to the logic riddle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61c8b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 ms, sys: 73 µs, total: 20.5 ms\n",
      "Wall time: 1.19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nNo, absence of proof is not the same as proof of absence. Absence of proof means that there is no evidence to support a claim, while proof of absence means that there is evidence to support the claim that something does not exist.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LOGIC_QUESTION_FORM_2 = \"Is it true that the absence of a proof equates the proof of an absence?\"\n",
    "# Switching to the other question again\n",
    "llm(LOGIC_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b76c3",
   "metadata": {},
   "source": [
    "## Additional options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e00d1",
   "metadata": {},
   "source": [
    "When creating the semantic cache, you can specify a few other options such as the metric used to calculate the similarity (and, accordingly, a corresponding threshold for accepting a \"cache hit\").\n",
    "\n",
    "Here is an example which uses the L2 (Euclidean) metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ff91966",
   "metadata": {},
   "outputs": [],
   "source": [
    "anotherCassSemanticCache = CassandraSemanticCache(\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    embedding=myEmbedding,\n",
    "    table_name=f'semantic_cache_{llmProvider}',\n",
    "    distance_metric='l2',\n",
    "    score_threshold=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae7824",
   "metadata": {},
   "source": [
    "This cache builds on the same database table as the previous one, as can be seen e.g. with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68588ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_entry_id = 77add13036bcaa23c74ebf2ab2c56441-0e4d63bf605cd5f4329128fcbe38762d\n",
      "\n",
      "No, absence of proof is not the same as proof of absence. Absence of proof means that there is no evidence to support a claim, while proof of absence means that there is evidence to support the claim that something does not exist.\n"
     ]
    }
   ],
   "source": [
    "lookup = anotherCassSemanticCache.lookup_with_id_through_llm(\n",
    "    LOGIC_QUESTION_FORM_2,\n",
    "    llm,\n",
    ")\n",
    "if lookup:\n",
    "    cache_entry_id, response = lookup\n",
    "    print(f\"cache_entry_id = {cache_entry_id}\")\n",
    "    # `response` is a List[langchain.schema.output.Generation], so:\n",
    "    print(f\"\\n{response[0].text.strip()}\")\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375cd27",
   "metadata": {},
   "source": [
    "## Caching and Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca45f87f",
   "metadata": {},
   "source": [
    "The `CassandraCache` supports caching within chat-oriented LangChain abstractions such as `ChatOpenAI` as well:\n",
    "\n",
    "_(warning: the following is demonstrated **with OpenAI only** for the time being)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c99f73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ce63ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, supernovae can result in the formation of a black hole. A supernova occurs when a massive star reaches the end of its life and undergoes a catastrophic explosion. The explosion expels most of the star's material into space, while the core collapses under its own gravity.\n",
      "\n",
      "If the core of the star is massive enough, typically more than three times the mass of the Sun, it will collapse further and form a black hole. This collapse is so intense that it creates a region of space with an extremely strong gravitational pull, from which nothing, not even light, can escape. This region is known as a black hole.\n",
      "CPU times: user 26 ms, sys: 11.6 ms, total: 37.6 ms\n",
      "Wall time: 5.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(chat_llm.predict(\"Can supernovae result in a black hole?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70a4724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, supernovae can result in the formation of a black hole. A supernova occurs when a massive star reaches the end of its life and undergoes a catastrophic explosion. The explosion expels most of the star's material into space, while the core collapses under its own gravity.\n",
      "\n",
      "If the core of the star is massive enough, typically more than three times the mass of the Sun, it will collapse further and form a black hole. This collapse is so intense that it creates a region of space with an extremely strong gravitational pull, from which nothing, not even light, can escape. This region is known as a black hole.\n",
      "CPU times: user 25.6 ms, sys: 311 µs, total: 26 ms\n",
      "Wall time: 686 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Expect a much faster response:\n",
    "print(chat_llm.predict(\"Is it possible that black holes come from big exploding stars?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aad310",
   "metadata": {},
   "source": [
    "(Actually, every object which inherits from the LangChain `Generation` class can be seamlessly store and retrieved in this cache.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306fe722",
   "metadata": {},
   "source": [
    "## Stale entry control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b39f20",
   "metadata": {},
   "source": [
    "### Time-To-Live (TTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da299a37",
   "metadata": {},
   "source": [
    "You can configure a time-to-live property of the cache, with the effect of automatic eviction of cached entries after a certain time.\n",
    "\n",
    "Setting `langchain.llm_cache` to the following will have the effect that entries vanish in an hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdae9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheWithTTL = CassandraSemanticCache(\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    embedding=myEmbedding,\n",
    "    table_name=f'semantic_cache_{llmProvider}',\n",
    "    ttl_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5848837",
   "metadata": {},
   "source": [
    "### Manual cache eviction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79682fb4",
   "metadata": {},
   "source": [
    "Alternatively, you can invalidate individual entries one at a time, just like you saw for the exact-match `CassandraCache` cache.\n",
    "\n",
    "But this is an index based on sentence similarity, so this time the procedure has two steps: **first**, a lookup to find the _id_ of the matching document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fce3834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a1339bc659790da078a4352c05bf422-0e4d63bf605cd5f4329128fcbe38762d\n"
     ]
    }
   ],
   "source": [
    "lookup = cassSemanticCache.lookup_with_id_through_llm(SPIDER_QUESTION_FORM_1, llm)\n",
    "if lookup:\n",
    "    cache_entry_id, response = lookup\n",
    "    print(cache_entry_id)\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43aa54e",
   "metadata": {},
   "source": [
    "_you can see that querying for another form for the \"same\" question will result in the same id:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72bb8b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a1339bc659790da078a4352c05bf422-0e4d63bf605cd5f4329128fcbe38762d\n"
     ]
    }
   ],
   "source": [
    "lookup2 = cassSemanticCache.lookup_with_id_through_llm(SPIDER_QUESTION_FORM_2, llm)\n",
    "if lookup2:\n",
    "    cache_entry_id2, response2 = lookup2\n",
    "    print(cache_entry_id2)\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeaa1a7",
   "metadata": {},
   "source": [
    "and **second**, the document id is used in the actual cache eviction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a95abd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.delete_by_document_id(cache_entry_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2f83d",
   "metadata": {},
   "source": [
    "As a check, try asking that question again and note the cell execution time (_you can also try re-running the above lookup cell..._):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9b376f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 ms, sys: 432 µs, total: 10.4 ms\n",
      "Wall time: 1.19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes, although some have fewer or more.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c822ac",
   "metadata": {},
   "source": [
    "### Whole-cache deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946aa8bb",
   "metadata": {},
   "source": [
    "Lastly, as you have seen earlier, you can empty the cache entirely with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a4b5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
