{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9288bca1",
   "metadata": {},
   "source": [
    "# Semantic LLM caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152c180",
   "metadata": {},
   "source": [
    "_**NOTE:** this uses Cassandra's experimental \"Vector Search\" capability.\n",
    "Make sure you are connecting to a vector-enabled database for this demo._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8771872b",
   "metadata": {},
   "source": [
    "The Cassandra-backed \"semantic cache\" for prompt responses is imported like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8ce64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import CassandraSemanticCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7bc8f",
   "metadata": {},
   "source": [
    "As usual, a database connection is needed to access Cassandra. The following assumes\n",
    "that a _vector-search-capable Cassandra cluster_ is running locally. Adjust as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb1a050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cqlsession import getCQLSession, getCQLKeyspace\n",
    "cqlMode = 'astra_db' # 'astra_db'/'local'\n",
    "session = getCQLSession(mode=cqlMode)\n",
    "keyspace = getCQLKeyspace(mode=cqlMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9f6d2",
   "metadata": {},
   "source": [
    "An embedding function and an LLM are needed.\n",
    "\n",
    "Below is the logic to instantiate the LLM and embeddings of choice. We choose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a946b81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM+embeddings from OpenAI\n"
     ]
    }
   ],
   "source": [
    "from llm_choice import suggestLLMProvider\n",
    "\n",
    "llmProvider = suggestLLMProvider()\n",
    "# (Alternatively set llmProvider to 'VertexAI', 'OpenAI' ... manually if you have credentials)\n",
    "\n",
    "if llmProvider == 'VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    from langchain.embeddings import VertexAIEmbeddings\n",
    "    llm = VertexAI()\n",
    "    myEmbedding = VertexAIEmbeddings()\n",
    "    print('LLM+embeddings from VertexAI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)\n",
    "    myEmbedding = OpenAIEmbeddings()\n",
    "    print('LLM+embeddings from OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c08eff",
   "metadata": {},
   "source": [
    "**Note**: for the time being you have to explicitly _turn on this experimental flag_ on the `cassio` side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58afd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassio\n",
    "cassio.globals.enableExperimentalVectorSearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a5ae3",
   "metadata": {},
   "source": [
    "## Create the cache\n",
    "\n",
    "At this point you can instantiate the semantic cache.\n",
    "\n",
    "_Note: in the following it is made clear, through the `table_name_prefix` parameter, that different embeddings will require separate tables. This is done here to avoid mismatches when running this demo over and over with varying embedding functions: in most applications, where a single embedding suffices, there's no need to be this finicky._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f04489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache = CassandraSemanticCache(\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    embedding=myEmbedding,\n",
    "    table_name_prefix=f'semantic_cache_{llmProvider}_',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c6159",
   "metadata": {},
   "source": [
    "Make sure the cache starts empty with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc165a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.clear_through_llm(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8fe19e",
   "metadata": {},
   "source": [
    "Configure the cache at a LangChain global level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ac5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.llm_cache = cassSemanticCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed086e91",
   "metadata": {},
   "source": [
    "## Use the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca683a93",
   "metadata": {},
   "source": [
    "Now try submitting a few prompts to the LLM and pay attention to the response times.\n",
    "\n",
    "If the LLM is actually run, they should be the order of a few seconds; but in case of a cache hit, it will be way less than a second.\n",
    "\n",
    "Notice that you get a cache hit even after rephrasing the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c051cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 ms, sys: 2.34 ms, total: 14.7 ms\n",
      "Wall time: 2.18 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_1 = \"How many eyes do spiders have?\"\n",
    "# A new question should take long\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71181e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.22 ms, sys: 642 Âµs, total: 5.86 ms\n",
      "Wall time: 263 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Second time, very same question, this should be quick\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc6d95b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.95 ms, sys: 1.04 ms, total: 6.98 ms\n",
      "Wall time: 436 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_2 = \"How many eyes does a spider generally have?\"\n",
    "# Just a rephrasing: but it's the same question,\n",
    "# so it will just take the time to evaluate embeddings\n",
    "llm(SPIDER_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eedbd72",
   "metadata": {},
   "source": [
    "Time for a really new question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fcdae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 ms, sys: 0 ns, total: 11.1 ms\n",
      "Wall time: 2.39 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nNo, absence of proof is not the same as proof of absence.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LOGIC_QUESTION_FORM_1 = \"Is absence of proof the same as proof of absence?\"\n",
    "# A totally new question\n",
    "llm(LOGIC_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1951f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.45 ms, sys: 1.29 ms, total: 7.74 ms\n",
      "Wall time: 589 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_3 = \"How many eyes are on the head of a typical spider?\"\n",
    "# Trying to catch the cache off-guard :)\n",
    "llm(SPIDER_QUESTION_FORM_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61c8b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 ms, sys: 1.17 ms, total: 13.7 ms\n",
      "Wall time: 537 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nNo, absence of proof is not the same as proof of absence.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LOGIC_QUESTION_FORM_2 = \"Is it true that the absence of a proof equates the proof of an absence?\"\n",
    "# Switching to the other question again\n",
    "llm(LOGIC_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b76c3",
   "metadata": {},
   "source": [
    "## Additional options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e00d1",
   "metadata": {},
   "source": [
    "When creating the semantic cache, you can specify a few other options such as the metric used to calculate the similarity and the number of entries to retrieve in the ANN step (i.e. those on which the exact requested metric is computed for the final filtering). Here is an example which uses the L2 metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ff91966",
   "metadata": {},
   "outputs": [],
   "source": [
    "anotherCassSemanticCache = CassandraSemanticCache(\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    embedding=myEmbedding,\n",
    "    distance_metric='l2',\n",
    "    score_threshold=0.4,\n",
    "    num_rows_to_fetch=12,\n",
    "    table_name_prefix=f'semantic_cache_{llmProvider}_',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae7824",
   "metadata": {},
   "source": [
    "This cache builds on the same database table as the previous one, as can be seen e.g. with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68588ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77add13036bcaa23c74ebf2ab2c56441\n",
      "[Generation(text='\\n\\nNo, absence of proof is not the same as proof of absence.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nNo, absence of proof is not the same as proof of absence.', generation_info={'finish_reason': 'stop', 'logprobs': None})]\n"
     ]
    }
   ],
   "source": [
    "lookup = anotherCassSemanticCache.lookup_with_id_through_llm(\n",
    "    LOGIC_QUESTION_FORM_2,\n",
    "    llm,\n",
    ")\n",
    "if lookup:\n",
    "    docId, response = lookup\n",
    "    print(docId)\n",
    "    print(response)\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306fe722",
   "metadata": {},
   "source": [
    "## Stale entry control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b39f20",
   "metadata": {},
   "source": [
    "### Time-To-Live (TTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da299a37",
   "metadata": {},
   "source": [
    "You can configure a time-to-live property of the cache, with the effect of automatic eviction of cached entries after a certain time.\n",
    "\n",
    "Setting `langchain.llm_cache` to the following will have the effect that entries vanish in an hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdae9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheWithTTL = CassandraSemanticCache(\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    embedding=myEmbedding,\n",
    "    ttl_seconds=15,\n",
    "    table_name_prefix=f'semantic_cache_{llmProvider}_',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5848837",
   "metadata": {},
   "source": [
    "### Manual cache eviction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79682fb4",
   "metadata": {},
   "source": [
    "Alternatively, you can invalidate individual entries one at a time, just like you saw for the exact-match `CassandraCache` cache.\n",
    "\n",
    "But this is an index based on sentence similarity, so this time the procedure has two steps: **first**, a lookup to find the _id_ of the matching document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fce3834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a1339bc659790da078a4352c05bf422\n"
     ]
    }
   ],
   "source": [
    "lookup = cassSemanticCache.lookup_with_id_through_llm(SPIDER_QUESTION_FORM_1, llm)\n",
    "if lookup:\n",
    "    docId, response = lookup\n",
    "    print(docId)\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43aa54e",
   "metadata": {},
   "source": [
    "_you can see that querying for another form for the \"same\" question will result in the same id:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72bb8b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a1339bc659790da078a4352c05bf422\n"
     ]
    }
   ],
   "source": [
    "lookup2 = cassSemanticCache.lookup_with_id_through_llm(SPIDER_QUESTION_FORM_2, llm)\n",
    "if lookup:\n",
    "    docId2, response2 = lookup2\n",
    "    print(docId2)\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeaa1a7",
   "metadata": {},
   "source": [
    "and **second**, the document id is used in the actual cache eviction (again, you have to additionally provide the LLM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a95abd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.delete_by_document_id_through_llm(docId, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2f83d",
   "metadata": {},
   "source": [
    "As a check, try asking that question again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9b376f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 ms, sys: 1.21 ms, total: 23.9 ms\n",
      "Wall time: 1.27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders generally have eight eyes, but some have fewer.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c822ac",
   "metadata": {},
   "source": [
    "### Whole-cache deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946aa8bb",
   "metadata": {},
   "source": [
    "Lastly, as you have seen earlier, you can empty the cache entirely, for a given LLM, with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a4b5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.clear_through_llm(llm=llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
