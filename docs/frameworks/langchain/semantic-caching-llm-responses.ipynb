{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c443a80",
   "metadata": {},
   "source": [
    "# Semantic LLM caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f3f054",
   "metadata": {},
   "source": [
    "_**NOTE:** this uses Cassandra's experimental \"Vector Similarity Search\" capability.\n",
    "At the moment, this is obtained by building and running an early alpha from a specific branch of the codebase._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d8648",
   "metadata": {},
   "source": [
    "The Cassandra-backed \"semantic cache\" for prompt responses is imported like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68457534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import CassandraSemanticCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8e882",
   "metadata": {},
   "source": [
    "As usual, a database connection is needed to access Cassandra. The following assumes\n",
    "that a _vector-search-capable Cassandra cluster_ is running locally. Adjust as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "016e87b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cqlsession import getLocalSession, getLocalKeyspace\n",
    "localSession = getLocalSession()\n",
    "localKeyspace = getLocalKeyspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6642e24b",
   "metadata": {},
   "source": [
    "An embedding function and an LLM are needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d195171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "myEmbedding = OpenAIEmbeddings()\n",
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a96e7",
   "metadata": {},
   "source": [
    "**Note**: for the time being you have to explicitly _turn on this experimental flag_ on the `cassio` side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "886a6a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassio\n",
    "cassio.globals.enableExperimentalVectorSearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba38f97",
   "metadata": {},
   "source": [
    "## Create the cache\n",
    "\n",
    "At this point you can instantiate the semantic cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a790b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache = CassandraSemanticCache(\n",
    "    session=localSession,\n",
    "    keyspace=localKeyspace,\n",
    "    embedding=myEmbedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147c3d7",
   "metadata": {},
   "source": [
    "Make sure the cache starts empty with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcc56748",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.clear_through_llm(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0874b6bb",
   "metadata": {},
   "source": [
    "Configure the cache at a LangChain global level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caf519a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.llm_cache = cassSemanticCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1550fc99",
   "metadata": {},
   "source": [
    "## Use the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ae528",
   "metadata": {},
   "source": [
    "Now try submitting a few prompts to the LLM and pay attention to the response times.\n",
    "\n",
    "If the LLM is actually run, they should be the order of a few seconds; but in case of a cache hit, it will be way less than a second.\n",
    "\n",
    "Notice that you get a cache hit even after rephrasing the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da49f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.6 ms, sys: 0 ns, total: 24.6 ms\n",
      "Wall time: 1.27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders have eight eyes.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_1 = \"How many eyes do spiders have?\"\n",
    "# A new question should take long\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c07c2248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 ms, sys: 0 ns, total: 15.7 ms\n",
      "Wall time: 114 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders have eight eyes.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Second time, very same question, this should be quick\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0665f439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 ms, sys: 0 ns, total: 18.2 ms\n",
      "Wall time: 571 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders have eight eyes.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_2 = \"How many eyes does a spider generally have?\"\n",
    "# Just a rephrasing: but it's the same question, so ...\n",
    "llm(SPIDER_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89360589",
   "metadata": {},
   "source": [
    "Time for a really new question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c13d18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.6 ms, sys: 0 ns, total: 31.6 ms\n",
      "Wall time: 1.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nNo, absence of proof is not the same as proof of absence.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LOGIC_QUESTION_FORM_1 = \"Is absence of proof the same as proof of absence?\"\n",
    "# A totally new question\n",
    "llm(LOGIC_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b243e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.3 ms, sys: 327 Âµs, total: 30.7 ms\n",
      "Wall time: 573 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders have eight eyes.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_3 = \"How many eyes are on the head of a typical spider?\"\n",
    "# Trying to catch the cache off-guard :)\n",
    "llm(SPIDER_QUESTION_FORM_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c183430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 ms, sys: 0 ns, total: 20.8 ms\n",
      "Wall time: 629 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nNo, absence of proof is not the same as proof of absence.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LOGIC_QUESTION_FORM_2 = \"Is it true that the absence of a proof equates the proof of an absence?\"\n",
    "# Switching to the other question again\n",
    "llm(LOGIC_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901dc12e",
   "metadata": {},
   "source": [
    "## Additional options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bc7bf",
   "metadata": {},
   "source": [
    "When creating the semantic cache, you can specify a few other options such as the metric used to calculate the similarity and the number of entries to retrieve in the ANN step (i.e. those on which the exact requested metric is computed for the final filtering). Here is an example which uses the L2 metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd40f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "anotherCassSemanticCache = CassandraSemanticCache(\n",
    "    session=localSession,\n",
    "    keyspace=localKeyspace,\n",
    "    embedding=myEmbedding,\n",
    "    distance_metric='l2',\n",
    "    score_threshold=0.35,\n",
    "    num_rows_to_fetch=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac4c8d",
   "metadata": {},
   "source": [
    "This cache builds on the same database table as the previous one, as can be seen e.g. with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71d8433c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77add13036bcaa23c74ebf2ab2c56441\n",
      "[Generation(text='\\n\\nNo, absence of proof is not the same as proof of absence.', generation_info=None), Generation(text='\\n\\nNo, absence of proof is not the same as proof of absence.', generation_info=None)]\n"
     ]
    }
   ],
   "source": [
    "lookup = anotherCassSemanticCache.lookup_with_id_through_llm(\n",
    "    LOGIC_QUESTION_FORM_2,\n",
    "    llm,\n",
    ")\n",
    "if lookup:\n",
    "    docId, response = lookup\n",
    "    print(docId)\n",
    "    print(response)\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7fdea",
   "metadata": {},
   "source": [
    "## Stale entry control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712565a5",
   "metadata": {},
   "source": [
    "### Time-To-Live (TTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2be1d2",
   "metadata": {},
   "source": [
    "You can configure a time-to-live property of the cache, with the effect of automatic eviction of cached entries after a certain time.\n",
    "\n",
    "Setting `langchain.llm_cache` to the following will have the effect that entries vanish in an hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f317200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheWithTTL = CassandraSemanticCache(\n",
    "    session=localSession,\n",
    "    keyspace=localKeyspace,\n",
    "    embedding=myEmbedding,\n",
    "    ttl_seconds=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f6c99",
   "metadata": {},
   "source": [
    "### Manual cache eviction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fcd7e9",
   "metadata": {},
   "source": [
    "Alternatively, you can invalidate individual entries one at a time, just like you saw for the exact-match `CassandraCache` cache.\n",
    "\n",
    "But this is an index based on sentence similarity, so this time the procedure has two steps: **first**, a lookup to find the _id_ of the matching document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30568083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a1339bc659790da078a4352c05bf422\n"
     ]
    }
   ],
   "source": [
    "lookup = cassSemanticCache.lookup_with_id_through_llm(SPIDER_QUESTION_FORM_1, llm)\n",
    "if lookup:\n",
    "    docId, response = lookup\n",
    "    print(docId)\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd5950",
   "metadata": {},
   "source": [
    "_you can see that querying for another form for the \"same\" question will result in the same id:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbeb022a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a1339bc659790da078a4352c05bf422\n"
     ]
    }
   ],
   "source": [
    "lookup2 = cassSemanticCache.lookup_with_id_through_llm(SPIDER_QUESTION_FORM_2, llm)\n",
    "if lookup:\n",
    "    docId2, response2 = lookup2\n",
    "    print(docId2)\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb118c4",
   "metadata": {},
   "source": [
    "and **second**, the document id is used in the actual cache eviction (again, you have to additionally provide the LLM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a807fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.delete_by_document_id_through_llm(docId, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57136613",
   "metadata": {},
   "source": [
    "As a check, try asking that question again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9185254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.8 ms, sys: 630 Âµs, total: 22.4 ms\n",
      "Wall time: 774 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders have eight eyes.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Trying to catch the cache off-guard :)\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf9194",
   "metadata": {},
   "source": [
    "### Whole-cache deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759924c",
   "metadata": {},
   "source": [
    "Lastly, as you have seen earlier, you can empty the cache entirely, for a given LLM, with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9976e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.clear_through_llm(llm=llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
