{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3afdf65",
   "metadata": {},
   "source": [
    "# Summary Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caaf19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import CassandraChatMessageHistory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4ea355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/cassandra/datastax/cloud/__init__.py:173: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n",
      "  ssl_context = SSLContext(PROTOCOL_TLS)\n",
      "/home/stefano/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/cassandra/io/asyncorereactor.py:347: DeprecationWarning: ssl.match_hostname() is deprecated\n",
      "  self._connect_socket()\n"
     ]
    }
   ],
   "source": [
    "from cqlsession import getCQLSession, getCQLKeyspace\n",
    "astraSession = getCQLSession()\n",
    "astraKeyspace = getCQLKeyspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81911a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = CassandraChatMessageHistory(\n",
    "    session_id='summarized-conversation-4321',\n",
    "    session=astraSession,\n",
    "    keyspace='langchain',\n",
    "    ttl_seconds = 3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34baf0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2294ea0",
   "metadata": {},
   "source": [
    "Below is the logic to instantiate the LLM of choice. We choose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a814761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/google/api_core/operations_v1/abstract_operations_client.py:17: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  from distutils import util\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM from VertexAI\n"
     ]
    }
   ],
   "source": [
    "from llm_choice import suggestLLMProvider\n",
    "\n",
    "llmProvider = suggestLLMProvider()\n",
    "# (Alternatively set llmProvider to 'VertexAI', 'OpenAI' ... manually if you have credentials)\n",
    "\n",
    "if llmProvider == 'VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    llm = VertexAI()\n",
    "    print('LLM from VertexAI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    from langchain.llms import OpenAI\n",
    "    llm = OpenAI()\n",
    "    print('LLM from OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1cd1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    chat_memory=message_history,\n",
    "    max_token_limit=180,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ed0fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryConversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7c97b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Tell me about William Tell, the hero\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'William Tell is a legendary hero of Switzerland.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryConversation.predict(input='Tell me about William Tell, the hero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c02ebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Tell me about William Tell, the hero\n",
      "AI: William Tell is a legendary hero of Switzerland.\n",
      "Human: Any relation to the apple from Cupertino?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No, the Apple Inc. is not named after William Tell.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryConversation.predict(\n",
    "    input=\"Any relation to the apple from Cupertino?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b1b82cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.moving_summary_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "891b5d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Tell me about William Tell, the hero\n",
      "AI: William Tell is a legendary hero of Switzerland.\n",
      "Human: Any relation to the apple from Cupertino?\n",
      "AI: No, the Apple Inc. is not named after William Tell.\n",
      "Human: I heard the two apples may be in fact the very same fruit.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I do not know.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryConversation.predict(\n",
    "    input=\"I heard the two apples may be in fact the very same fruit.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea75de13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.moving_summary_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1f5ba0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human asks about William Tell, the hero of Switzerland. The AI says that William Tell is not related to the Apple Inc. The human then asks if the two apples may be in fact the very same fruit. The AI says that it does not know.\n"
     ]
    }
   ],
   "source": [
    "print(memory.predict_new_summary(\n",
    "    memory.chat_memory.messages,\n",
    "    memory.moving_summary_buffer,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c507b6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Tell me about William Tell, the hero\n",
      "AI: William Tell is a legendary hero of Switzerland.\n",
      "Human: Any relation to the apple from Cupertino?\n",
      "AI: No, the Apple Inc. is not named after William Tell.\n",
      "Human: I heard the two apples may be in fact the very same fruit.\n",
      "AI: I do not know.\n",
      "Human: Ok, enough with apples. Let's talk starfish now.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Starfish are marine echinoderms.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryConversation.predict(\n",
    "    input='Ok, enough with apples. Let\\'s talk starfish now.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84d1b8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.moving_summary_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69188782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human asks about William Tell, the hero. The AI says that William Tell is a legendary hero of Switzerland. The human asks if there is any relation to the apple from Cupertino. The AI says that the Apple Inc. is not named after William Tell. The human says that they heard the two apples may be in fact the very same fruit. The AI says that they do not know. The human says that they are done talking about apples and wants to talk about starfish now. The AI says that starfish are marine echinoderms.\n"
     ]
    }
   ],
   "source": [
    "print(memory.predict_new_summary(\n",
    "    memory.chat_memory.messages,\n",
    "    memory.moving_summary_buffer,\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
