{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab-specific setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a Database, a Secure Connect Bundle and a Database Token. See [DB Setup](https://cassio.org/db_setup/) on cassio.org for details, **paying attention to the notes for Colab users**.\n",
    "Get ready to upload the Bundle and supply the Token string.\n",
    "\n",
    "Likewise, ensure you have the necessary secret for the LLM provider of your choice: you'll be asked to input it shortly. See [API Setup](https://cassio.org/api_setup/) on cassio.org for details, **paying attention to the notes for Colab users**.\n",
    "\n",
    "_Note: this colab is autogenerated from a [regular Jupyter notebook](https://cassio.org/frameworks/langchain/caching-llm-responses/) hosted on cassio.org. Run all cells in this section to complete the setup before moving on to the demo content proper._\n",
    "\n",
    "_Note: you can work with your own Cassandra cluster instead of Astra DB, provided it is reachable from the cloud: check the commented code below for the case `cqlMode=\"local\"` and consult [cassio.org](https://cassio.org/frameworks/langchain/setup/#database-choice) for more details._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required dependencies\n",
    "! pip install \\\n",
    "    \"git+https://github.com/hemidactylus/langchain@cassio#egg=langchain\" \\\n",
    "    \"cassandra-driver>=3.28.0\" \\\n",
    "    \"cassio>=0.0.2\" \\\n",
    "    \"google-cloud-aiplatform>=1.25.0\" \\\n",
    "    \"jupyter>=1.0.0\" \\\n",
    "    \"openai==0.27.7\" \\\n",
    "    \"python-dotenv==1.0.0\" \\\n",
    "    \"tensorflow-cpu==2.12.0\" \\\n",
    "    \"tiktoken==0.4.0\" \\\n",
    "    \"transformers>=4.29.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your database keyspace name:\n",
    "ASTRA_DB_KEYSPACE = input('Your Astra DB Keyspace name: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your Astra DB token string, the one starting with \"AstraCS:...\"\n",
    "ASTRA_DB_APPLICATION_TOKEN = input('Your Astra DB Token: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your Secure Connect Bundle zipfile:\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "print('Please upload your Secure Connect Bundle')\n",
    "uploaded = files.upload()\n",
    "if uploaded:\n",
    "    astraBundleFileTitle = list(uploaded.keys())[0]\n",
    "    ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your secret(s) for LLM access (key names must match `providerValidator` in `llm_choice`)\n",
    "llmProvider = 'OpenAI'  # 'VertexAI'\n",
    "if llmProvider == 'OpenAI':\n",
    "    apiSecret = input(f'Your secret for LLM provider \"{llmProvider}\": ')\n",
    "    os.environ['OPENAI_API_KEY'] = apiSecret\n",
    "elif llmProvider == 'VertexAI':\n",
    "    # we need a json file\n",
    "    print(f'Please upload your Service Account JSON for the LLM provider \"{llmProvider}\":')\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    if uploaded:\n",
    "        vertexAIJsonFileTitle = list(uploaded.keys())[0]\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(os.getcwd(), vertexAIJsonFileTitle)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'No file uploaded. Please re-run the cell.'\n",
    "        )\n",
    "else:\n",
    "    raise ValueError('Unknown/unsupported LLM Provider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab-specific override of helper functions\n",
    "from cassandra.cluster import (\n",
    "    Cluster,\n",
    ")\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "ASTRA_DB_CLIENT_ID = 'token'\n",
    "\n",
    "\n",
    "def getCQLSession(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        cluster = Cluster(\n",
    "            cloud={\n",
    "                \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
    "            },\n",
    "            auth_provider=PlainTextAuthProvider(\n",
    "                ASTRA_DB_CLIENT_ID,\n",
    "                ASTRA_DB_APPLICATION_TOKEN,\n",
    "            ),\n",
    "        )\n",
    "        astraSession = cluster.connect()\n",
    "        return astraSession\n",
    "    # elif mode == 'local':\n",
    "    #     cluster = Cluster(\n",
    "    #         ['192.168.0.1', '192.168.0.2'],\n",
    "    #         auth_provider=PlainTextAuthProvider(\n",
    "    #             \"username\",\n",
    "    #             \"password!\",\n",
    "    #         ),\n",
    "    #     )\n",
    "    #     # See https://docs.datastax.com/en/developer/python-driver/latest/getting_started/#connecting-to-cassandra for more options\n",
    "    #     localSession = cluster.connect()\n",
    "    #     return localSession\n",
    "    else:\n",
    "        raise ValueError('Unknown CQL Session mode')\n",
    "\n",
    "def getCQLKeyspace(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        return ASTRA_DB_KEYSPACE\n",
    "    # elif mode == 'local':\n",
    "    #     return <NAME_OF_YOUR_LOCAL_KEYSPACE>\n",
    "    else:\n",
    "        raise ValueError('Unknown CQL Session mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab preamble completed\n",
    "\n",
    "The following cells constitute the demo notebook proper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84094469",
   "metadata": {},
   "source": [
    "# Caching LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddaf62",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use Cassandra for a basic prompt/response cache.\n",
    "\n",
    "Such a cache prevents running an LLM invocation more than once for the very same prompt, thus saving on latency and token usage. The cache retrieval logic is based on an exact match, as will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec81edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import CassandraCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e24fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the DB connection\n",
    "cqlMode = 'astra_db'\n",
    "session = getCQLSession(mode=cqlMode)\n",
    "keyspace = getCQLKeyspace(mode=cqlMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65bff51",
   "metadata": {},
   "source": [
    "Create a `CassandraCache` and configure it globally for LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "445307fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.llm_cache = CassandraCache(\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b7e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.llm_cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b619bb",
   "metadata": {},
   "source": [
    "Below is the logic to instantiate the LLM of choice. We choose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a533119f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM from OpenAI\n"
     ]
    }
   ],
   "source": [
    "# creation of the LLM resources\n",
    "\n",
    "\n",
    "if llmProvider == 'VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    llm = VertexAI()\n",
    "    print('LLM from VertexAI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    from langchain.llms import OpenAI\n",
    "    llm = OpenAI()\n",
    "    print('LLM from OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f17cce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 ms, sys: 1.39 ms, total: 19.4 ms\n",
      "Wall time: 2.25 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes, although some have six or fewer.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_1 = \"How many eyes do spiders have?\"\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47bc4bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.2 ms, sys: 0 ns, total: 2.2 ms\n",
      "Wall time: 32.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes, although some have six or fewer.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# This time we expect a much shorter answer time\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88d657dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 ms, sys: 992 Âµs, total: 14.4 ms\n",
      "Wall time: 1.17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders typically have eight eyes.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_2 = \"How many eyes do spiders generally have?\"\n",
    "# This will again take 1-2 seconds, being a different string\n",
    "llm(SPIDER_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048a336",
   "metadata": {},
   "source": [
    "### Stale entry control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca8cd5",
   "metadata": {},
   "source": [
    "#### Time-To-Live (TTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de2adf",
   "metadata": {},
   "source": [
    "You can configure a time-to-live property of the cache, with the effect of automatic eviction of cached entries after a certain time.\n",
    "\n",
    "Setting `langchain.llm_cache` to the following will have the effect that entries vanish in an hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faec79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheWithTTL = CassandraCache(\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    ttl_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8ec77",
   "metadata": {},
   "source": [
    "#### Manual cache eviction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b414d43",
   "metadata": {},
   "source": [
    "Alternatively, you can invalidate cached entries one at a time - for that, you'll need to provide the very LLM this entry is associated to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56df34d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.96 ms, sys: 0 ns, total: 3.96 ms\n",
      "Wall time: 31.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders typically have eight eyes.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(SPIDER_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d42dcb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.llm_cache.delete_through_llm(SPIDER_QUESTION_FORM_2, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed25580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.78 ms, sys: 1.18 ms, total: 11 ms\n",
      "Wall time: 1.45 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders typically have eight eyes, though some species may have fewer or more.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(SPIDER_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3193c67",
   "metadata": {},
   "source": [
    "#### Whole-cache deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d74e6",
   "metadata": {},
   "source": [
    "As you might have seen at the beginning of this notebook, you can also clear the cache entirely: **all** stored entries, for all models, will be evicted at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff899ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.llm_cache.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
