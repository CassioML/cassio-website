{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic LLM caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab-specific setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a DB, a bundle and a token. See [BD setup](https://cassio.org/db_setup/) on cassio.org for details.\n",
    "Get ready to upload the bundle and supply the token string.\n",
    "\n",
    "Likewise, ensure you have the necessary secret for a LLM provider: you'll be asked to input it shortly. See [Api setup](https://cassio.org/api_setup/) on cassio.org for details.\n",
    "\n",
    "_Note: this colab is autogenerated from a regular Jupyter notebook hosted by cassio.org. Run all cells in this section to complete the setup before moving on to the demo content proper._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required dependencies\n",
    "! pip install \\\n",
    "    \"git+https://github.com/hemidactylus/langchain@cassio#egg=langchain\" \\\n",
    "    \"git+https://github.com/datastax/python-driver.git@cep-vsearch#egg=cassandra-driver\" \\\n",
    "    \"cassio>=0.0.2\" \\\n",
    "    \"feast>=0.26\" \\\n",
    "    \"google-cloud-aiplatform>=1.25.0\" \\\n",
    "    \"jupyter>=1.0.0\" \\\n",
    "    \"openai==0.27.7\" \\\n",
    "    \"python-dotenv==1.0.0\" \\\n",
    "    \"tensorflow-cpu==2.12.0\" \\\n",
    "    \"tiktoken==0.4.0\" \\\n",
    "    \"transformers>=4.29.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your database keyspace name:\n",
    "ASTRA_DB_KEYSPACE = input('Your Astra DB Keyspace name: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your Astra DB token string, the one starting with \"AstraCS:...\"\n",
    "ASTRA_DB_APPLICATION_TOKEN = input('Your Astra DB Token: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your Secure Connect Bundle zipfile:\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "print('Please upload your Secure Connect Bundle')\n",
    "uploaded = files.upload()\n",
    "if uploaded:\n",
    "    astraBundleFileTitle = list(uploaded.keys())[0]\n",
    "    ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your secret(s) for LLM access (key names must match `providerValidator` in `llm_choice`)\n",
    "llmProvider = 'OpenAI'  # 'VertexAI'\n",
    "if llmProvider == 'OpenAI':\n",
    "    apiSecret = input(f'Your secret for LLM provider \"{llmProvider}\": ')\n",
    "    os.environ['OPENAI_API_KEY'] = apiSecret\n",
    "elif llmProvider == 'VertexAI':\n",
    "    # we need a json file\n",
    "    print(f'Please upload your Service Account JSON for the LLM provider \"{llmProvider}\":')\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    if uploaded:\n",
    "        vertexAIJsonFileTitle = list(uploaded.keys())[0]\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(os.getcwd(), vertexAIJsonFileTitle)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'No file uploaded. Please re-run the cell.'\n",
    "        )\n",
    "else:\n",
    "    raise ValueError('Unknown/unsupported LLM Provider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab-specific override of helper functions\n",
    "from cassandra.cluster import (\n",
    "    Cluster,\n",
    "    ExecutionProfile,\n",
    "    EXEC_PROFILE_DEFAULT,\n",
    "    ConsistencyLevel,\n",
    ")\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "ASTRA_DB_CLIENT_ID = 'token'\n",
    "\n",
    "\n",
    "def getCQLSession(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        profile = ExecutionProfile(\n",
    "            consistency_level=ConsistencyLevel.LOCAL_QUORUM,\n",
    "        )\n",
    "        #\n",
    "        cluster = Cluster(\n",
    "            cloud={\n",
    "                \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
    "            },\n",
    "            auth_provider=PlainTextAuthProvider(\n",
    "                ASTRA_DB_CLIENT_ID,\n",
    "                ASTRA_DB_APPLICATION_TOKEN,\n",
    "            ),\n",
    "            execution_profiles={EXEC_PROFILE_DEFAULT: profile},\n",
    "        )\n",
    "        astraSession = cluster.connect()\n",
    "        return astraSession\n",
    "    else:\n",
    "        raise ValueError('Unknown CQL Session mode')\n",
    "\n",
    "def getCQLKeyspace(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        return ASTRA_DB_KEYSPACE\n",
    "    else:\n",
    "        raise ValueError('Unknown CQL Session mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288bca1",
   "metadata": {},
   "source": [
    "# Semantic LLM caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152c180",
   "metadata": {},
   "source": [
    "_**NOTE:** this uses Cassandra's experimental \"Vector Search\" capability.\n",
    "Make sure you are connecting to a vector-enabled database for this demo._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8771872b",
   "metadata": {},
   "source": [
    "The Cassandra-backed \"semantic cache\" for prompt responses is imported like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8ce64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import CassandraSemanticCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7bc8f",
   "metadata": {},
   "source": [
    "As usual, a database connection is needed to access Cassandra. The following assumes\n",
    "that a _vector-search-capable Cassandra cluster_ is running locally. Adjust as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb1a050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the DB connection\n",
    "cqlMode = 'astra_db'\n",
    "session = getCQLSession(mode=cqlMode)\n",
    "keyspace = getCQLKeyspace(mode=cqlMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9f6d2",
   "metadata": {},
   "source": [
    "An embedding function and an LLM are needed.\n",
    "\n",
    "Below is the logic to instantiate the LLM and embeddings of choice. We choose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a946b81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM+embeddings from OpenAI\n"
     ]
    }
   ],
   "source": [
    "# creation of the LLM resources\n",
    "\n",
    "\n",
    "if llmProvider == 'VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    from langchain.embeddings import VertexAIEmbeddings\n",
    "    llm = VertexAI()\n",
    "    myEmbedding = VertexAIEmbeddings()\n",
    "    print('LLM+embeddings from VertexAI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)\n",
    "    myEmbedding = OpenAIEmbeddings()\n",
    "    print('LLM+embeddings from OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c08eff",
   "metadata": {},
   "source": [
    "**Note**: for the time being you have to explicitly _turn on this experimental flag_ on the `cassio` side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58afd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassio\n",
    "cassio.globals.enableExperimentalVectorSearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a5ae3",
   "metadata": {},
   "source": [
    "## Create the cache\n",
    "\n",
    "At this point you can instantiate the semantic cache.\n",
    "\n",
    "_Note: in the following it is made clear, through the `table_name_prefix` parameter, that different embeddings will require separate tables. This is done here to avoid mismatches when running this demo over and over with varying embedding functions: in most applications, where a single embedding suffices, there's no need to be this finicky._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f04489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache = CassandraSemanticCache(\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    embedding=myEmbedding,\n",
    "    table_name_prefix=f'semantic_cache_{llmProvider}_',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c6159",
   "metadata": {},
   "source": [
    "Make sure the cache starts empty with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc165a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.clear_through_llm(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8fe19e",
   "metadata": {},
   "source": [
    "Configure the cache at a LangChain global level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ac5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.llm_cache = cassSemanticCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed086e91",
   "metadata": {},
   "source": [
    "## Use the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca683a93",
   "metadata": {},
   "source": [
    "Now try submitting a few prompts to the LLM and pay attention to the response times.\n",
    "\n",
    "If the LLM is actually run, they should be the order of a few seconds; but in case of a cache hit, it will be way less than a second.\n",
    "\n",
    "Notice that you get a cache hit even after rephrasing the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c051cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 ms, sys: 0 ns, total: 13 ms\n",
      "Wall time: 2.12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_1 = \"How many eyes do spiders have?\"\n",
    "# A new question should take long\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71181e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.57 ms, sys: 1.89 ms, total: 7.46 ms\n",
      "Wall time: 130 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Second time, very same question, this should be quick\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc6d95b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.9 ms, sys: 0 ns, total: 9.9 ms\n",
      "Wall time: 343 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_2 = \"How many eyes does a spider generally have?\"\n",
    "# Just a rephrasing: but it's the same question,\n",
    "# so it will just take the time to evaluate embeddings\n",
    "llm(SPIDER_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eedbd72",
   "metadata": {},
   "source": [
    "Time for a really new question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fcdae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.92 ms, sys: 3.8 ms, total: 13.7 ms\n",
      "Wall time: 1.75 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nNo, absence of proof is not the same as proof of absence.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LOGIC_QUESTION_FORM_1 = \"Is absence of proof the same as proof of absence?\"\n",
    "# A totally new question\n",
    "llm(LOGIC_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1951f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.37 ms, sys: 1.06 ms, total: 9.44 ms\n",
      "Wall time: 311 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_3 = \"How many eyes are on the head of a typical spider?\"\n",
    "# Trying to catch the cache off-guard :)\n",
    "llm(SPIDER_QUESTION_FORM_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61c8b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.4 ms, sys: 2.42 ms, total: 24.9 ms\n",
      "Wall time: 518 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nNo, absence of proof is not the same as proof of absence.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LOGIC_QUESTION_FORM_2 = \"Is it true that the absence of a proof equates the proof of an absence?\"\n",
    "# Switching to the other question again\n",
    "llm(LOGIC_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b76c3",
   "metadata": {},
   "source": [
    "## Additional options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e00d1",
   "metadata": {},
   "source": [
    "When creating the semantic cache, you can specify a few other options such as the metric used to calculate the similarity and the number of entries to retrieve in the ANN step (i.e. those on which the exact requested metric is computed for the final filtering). Here is an example which uses the L2 metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ff91966",
   "metadata": {},
   "outputs": [],
   "source": [
    "anotherCassSemanticCache = CassandraSemanticCache(\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    embedding=myEmbedding,\n",
    "    distance_metric='l2',\n",
    "    score_threshold=0.4,\n",
    "    num_rows_to_fetch=12,\n",
    "    table_name_prefix=f'semantic_cache_{llmProvider}_',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae7824",
   "metadata": {},
   "source": [
    "This cache builds on the same database table as the previous one, as can be seen e.g. with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68588ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77add13036bcaa23c74ebf2ab2c56441\n",
      "[Generation(text='\\n\\nNo, absence of proof is not the same as proof of absence.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nThere is a difference between absence of proof and proof of absence. Absence of proof means that there is no evidence to support a claim. Proof of absence means that there is evidence that disproves a claim.', generation_info={'finish_reason': 'stop', 'logprobs': None})]\n"
     ]
    }
   ],
   "source": [
    "lookup = anotherCassSemanticCache.lookup_with_id_through_llm(\n",
    "    LOGIC_QUESTION_FORM_2,\n",
    "    llm,\n",
    ")\n",
    "if lookup:\n",
    "    docId, response = lookup\n",
    "    print(docId)\n",
    "    print(response)\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306fe722",
   "metadata": {},
   "source": [
    "## Stale entry control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b39f20",
   "metadata": {},
   "source": [
    "### Time-To-Live (TTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da299a37",
   "metadata": {},
   "source": [
    "You can configure a time-to-live property of the cache, with the effect of automatic eviction of cached entries after a certain time.\n",
    "\n",
    "Setting `langchain.llm_cache` to the following will have the effect that entries vanish in an hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdae9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheWithTTL = CassandraSemanticCache(\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    embedding=myEmbedding,\n",
    "    ttl_seconds=15,\n",
    "    table_name_prefix=f'semantic_cache_{llmProvider}_',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5848837",
   "metadata": {},
   "source": [
    "### Manual cache eviction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79682fb4",
   "metadata": {},
   "source": [
    "Alternatively, you can invalidate individual entries one at a time, just like you saw for the exact-match `CassandraCache` cache.\n",
    "\n",
    "But this is an index based on sentence similarity, so this time the procedure has two steps: **first**, a lookup to find the _id_ of the matching document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fce3834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a1339bc659790da078a4352c05bf422\n"
     ]
    }
   ],
   "source": [
    "lookup = cassSemanticCache.lookup_with_id_through_llm(SPIDER_QUESTION_FORM_1, llm)\n",
    "if lookup:\n",
    "    docId, response = lookup\n",
    "    print(docId)\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43aa54e",
   "metadata": {},
   "source": [
    "_you can see that querying for another form for the \"same\" question will result in the same id:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72bb8b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a1339bc659790da078a4352c05bf422\n"
     ]
    }
   ],
   "source": [
    "lookup2 = cassSemanticCache.lookup_with_id_through_llm(SPIDER_QUESTION_FORM_2, llm)\n",
    "if lookup:\n",
    "    docId2, response2 = lookup2\n",
    "    print(docId2)\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeaa1a7",
   "metadata": {},
   "source": [
    "and **second**, the document id is used in the actual cache eviction (again, you have to additionally provide the LLM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a95abd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.delete_by_document_id_through_llm(docId, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2f83d",
   "metadata": {},
   "source": [
    "As a check, try asking that question again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9b376f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 ms, sys: 0 ns, total: 15.8 ms\n",
      "Wall time: 1.23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes, but some have fewer or more.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c822ac",
   "metadata": {},
   "source": [
    "### Whole-cache deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946aa8bb",
   "metadata": {},
   "source": [
    "Lastly, as you have seen earlier, you can empty the cache entirely, for a given LLM, with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a4b5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.clear_through_llm(llm=llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
