{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic LLM response caching\n",
    "\n",
    "Save on tokens and latency with a LLM response cache based on semantic similarity (as opposed to exact match), powered by Vector Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab-specific setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a database available. This is a Colab notebook, so here it is assumed you will use\n",
    "an Astra DB cloud instance. For usage with an Apache Cassandra® cluster, switch to a local setup\n",
    "as instructed [on cassio.org](https://cassio.org/more_info/#use-a-local-vector-capable-cassandra).\n",
    "\n",
    "Get ready to supply the connection parameters: Database ID and Token string\n",
    "(see [Pre-requisites](https://cassio.org/start_here/#vector-database) on cassio.org for details.\n",
    "Remember you need a Token with role [Database Administrator](https://awesome-astra.github.io/docs/pages/astra/create-token/)).\n",
    "\n",
    "Likewise, ensure you have the necessary secret(s) for the LLM provider of your choice: you'll be asked to input it shortly\n",
    "(see [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org for details).\n",
    "\n",
    "_Note: this notebook is part of the CassIO documentation. Visit [this page on cassIO.org](https://cassio.org/frameworks/langchain/semantic-caching-llm-responses/)._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required dependencies\n",
    "! pip install -q --progress-bar off \\\n",
    "    \"cassio>=0.1.5\" \\\n",
    "    \"jupyter>=1.0.0\" \\\n",
    "    \"langchain>=0.1.16,<0.2\" \\\n",
    "    \"langchain-openai>=0.1.3,<0.2\" \\\n",
    "    \"langchain-google-vertexai>=1.16.0,<2\" \\\n",
    "    \"python-dotenv==1.0.1\" \n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f44ff",
   "metadata": {},
   "source": [
    "⚠️ **Do not mind a \"Your session crashed...\" message you may see.**\n",
    "\n",
    "It was us, making sure your kernel restarts with all the correct dependency versions. _You can now proceed with the notebook._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB connection parameters",
    "\n",
    "Example values (check your Astra dashboard):\n",
    "- Database ID example: `01234567-89ab-cdef-0123-456789abcdef`\n",
    "- Token example: `AstraCS:6gBhNmsk135....` (ensure it has a role of at least \"Database Administrator\")\n",
    "- _Keyspace. Optional, string if provided. Example:_ `default_keyspace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Input your Astra DB connection parameters and secrets:\n",
    "\n",
    "os.environ[\"ASTRA_DB_ID\"] = input(\"ASTRA_DB_ID = \")\n",
    "\n",
    "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = getpass(\"ASTRA_DB_APPLICATION_TOKEN = \")\n",
    "\n",
    "_keyspace = input(\"(Optional) ASTRA_DB_KEYSPACE = \")\n",
    "if _keyspace:\n",
    "    os.environ[\"ASTRA_DB_KEYSPACE\"] = _keyspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXCQ6T_Gjk0Oz"
   },
   "source": [
    "### LLM Provider\n",
    "\n",
    "In the cell below you can choose between **GCP Vertex AI** or **OpenAI** for your LLM services.\n",
    "(See [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org for more details).\n",
    "\n",
    "Make sure you set the `llmProvider` variable and supply the corresponding access secrets in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your secret(s) for LLM access:\n",
    "llmProvider = 'OpenAI'  # 'GCP_VertexAI', 'Azure_OpenAI'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "if llmProvider == 'OpenAI':\n",
    "    apiSecret = getpass(f'Your secret for LLM provider \"{llmProvider}\": ')\n",
    "    os.environ['OPENAI_API_KEY'] = apiSecret\n",
    "elif llmProvider == 'GCP_VertexAI':\n",
    "    # we need a json file\n",
    "    print(f'Please upload your Service Account JSON for the LLM provider \"{llmProvider}\":')\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    if uploaded:\n",
    "        vertexAIJsonFileTitle = list(uploaded.keys())[0]\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(os.getcwd(), vertexAIJsonFileTitle)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'No file uploaded. Please re-run the cell.'\n",
    "        )\n",
    "elif llmProvider == 'Azure_OpenAI':\n",
    "    # a few parameters must be input\n",
    "    apiSecret = input(f'Your API Key for LLM provider \"{llmProvider}\": ')\n",
    "    os.environ['AZURE_OPENAI_API_KEY'] = apiSecret\n",
    "    apiBase = input('The \"Base URL\" for your models (e.g. \"https://YOUR-RESOURCE-NAME.openai.azure.com\"): ')\n",
    "    os.environ['AZURE_OPENAI_ENDPOINT'] = apiBase\n",
    "    apiLLMDepl = input('The name of your LLM Deployment: ')\n",
    "    os.environ['AZURE_OPENAI_LLM_DEPLOYMENT'] = apiLLMDepl\n",
    "    apiLLMModel = input('The name of your LLM Model (e.g. \"gpt-4\"): ')\n",
    "    os.environ['AZURE_OPENAI_LLM_MODEL'] = apiLLMModel\n",
    "    apiEmbDepl = input('The name for your Embeddings Deployment: ')\n",
    "    os.environ['AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT'] = apiEmbDepl\n",
    "    apiEmbModel = input('The name of your Embedding Model (e.g. \"text-embedding-ada-002\"): ')\n",
    "    os.environ['AZURE_OPENAI_EMBEDDINGS_MODEL'] = apiEmbModel\n",
    "\n",
    "    # The following is probably not going to change for some time...\n",
    "    os.environ['AZURE_OPENAI_API_VERSION'] = '2023-03-15-preview'\n",
    "else:\n",
    "    raise ValueError('Unknown/unsupported LLM Provider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab preamble completed\n",
    "\n",
    "The following cells constitute the demo notebook proper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288bca1",
   "metadata": {},
   "source": [
    "# Semantic LLM response caching\n",
    "\n",
    "Save on tokens and latency with a LLM response cache based on semantic similarity (as opposed to exact match), powered by Vector Search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152c180",
   "metadata": {},
   "source": [
    "_**NOTE:** this uses Cassandra's \"Vector Search\" capability.\n",
    "Make sure you are connecting to a vector-enabled database for this demo._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8771872b",
   "metadata": {},
   "source": [
    "The Cassandra-backed \"semantic cache\" for prompt responses is imported like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8ce64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import CassandraSemanticCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578a87b",
   "metadata": {},
   "source": [
    "A database connection is needed. _(If on a Colab, the only supported option is the cloud service Astra DB.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9515d4af-9fc2-4c29-b9f2-5c90cd48cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ready to initialize the DB connection globally ...\n",
    "import os\n",
    "\n",
    "import cassio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ee44a4-8d6a-4b41-8a8f-7c93eed35034",
   "metadata": {},
   "source": [
    "Select your choice of database by editing this cell, if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe78b92-7194-4ee9-ba7f-cd308409212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_mode = \"astra_db\"  # only \"astra_db\" supported on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5597b3-a028-4d8f-ad6d-8eaf0c941dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if database_mode == \"astra_db\":\n",
    "    cassio.init(\n",
    "        database_id=os.environ[\"ASTRA_DB_ID\"],\n",
    "        token=os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"],\n",
    "        keyspace=os.environ.get(\"ASTRA_DB_KEYSPACE\"),  # this is optional\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "329d969b-6532-4dc4-a315-1a5438a59919",
   "metadata": {},
   "outputs": [],
   "source": [
    "if database_mode == \"cassandra\":\n",
    "    # Cassandra not supported on Colab - please define your own getCassandraCQLSession/getCassandraCQLKeyspace\n",
    "    cassio.init(\n",
    "        session=getCassandraCQLSession(),\n",
    "        keyspace=getCassandraCQLKeyspace(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9f6d2",
   "metadata": {},
   "source": [
    "An embedding function and an LLM are needed.\n",
    "\n",
    "Below is the logic to instantiate the LLM and embeddings of choice. We chose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a946b81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM+embeddings from OpenAI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# creation of the LLM resources\n",
    "\n",
    "\n",
    "if llmProvider == 'GCP_VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    from langchain.embeddings import VertexAIEmbeddings\n",
    "    llm = VertexAI()\n",
    "    myEmbedding = VertexAIEmbeddings()\n",
    "    print('LLM+embeddings from Vertex AI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    os.environ['OPENAI_API_TYPE'] = 'open_ai'\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = OpenAI(temperature=0)\n",
    "    myEmbedding = OpenAIEmbeddings()\n",
    "    print('LLM+embeddings from OpenAI')\n",
    "elif llmProvider == 'Azure_OpenAI':\n",
    "    os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "    os.environ['OPENAI_API_VERSION'] = os.environ['AZURE_OPENAI_API_VERSION']\n",
    "    os.environ['OPENAI_API_BASE'] = os.environ['AZURE_OPENAI_API_BASE']\n",
    "    os.environ['OPENAI_API_KEY'] = os.environ['AZURE_OPENAI_API_KEY']\n",
    "    from langchain.llms import AzureOpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = AzureOpenAI(temperature=0, model_name=os.environ['AZURE_OPENAI_LLM_MODEL'],\n",
    "                      engine=os.environ['AZURE_OPENAI_LLM_DEPLOYMENT'])\n",
    "    myEmbedding = OpenAIEmbeddings(model=os.environ['AZURE_OPENAI_EMBEDDINGS_MODEL'],\n",
    "                                   deployment=os.environ['AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT'])\n",
    "    print('LLM+embeddings from Azure OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a5ae3",
   "metadata": {},
   "source": [
    "## Create the cache\n",
    "\n",
    "At this point you can instantiate the semantic cache.\n",
    "\n",
    "_Note: in the following it is made clear, through the way the `table` parameter is constructed, that different embeddings will require separate tables. This is done here to avoid mismatches when running this demo over and over with varying embedding functions: in most applications, where a single choice of embedding is made, there's no need to be this finicky and you cal usually leave the table name to its default value._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f04489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache = CassandraSemanticCache(\n",
    "    session=None,\n",
    "    keyspace=None,\n",
    "    embedding=myEmbedding,\n",
    "    table_name=f'semantic_cache_{llmProvider}',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c6159",
   "metadata": {},
   "source": [
    "Make sure the cache starts empty with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc165a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8fe19e",
   "metadata": {},
   "source": [
    "Configure the cache at a LangChain global level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05ac5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.llm_cache = cassSemanticCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed086e91",
   "metadata": {},
   "source": [
    "## Use the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca683a93",
   "metadata": {},
   "source": [
    "Now try submitting a few prompts to the LLM and pay attention to the response times.\n",
    "\n",
    "If the LLM is actually run, they should be the order of a few seconds; but in case of a cache hit, it will be way less than a second.\n",
    "\n",
    "Notice that you get a cache hit even after rephrasing the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c051cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.9 ms, sys: 0 ns, total: 23.9 ms\n",
      "Wall time: 1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes, although some have fewer or more.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_1 = \"How many eyes do spiders have?\"\n",
    "# A new question should take long\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71181e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.44 ms, sys: 0 ns, total: 3.44 ms\n",
      "Wall time: 4.97 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes, although some have fewer or more.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Second time, very same question, this should be quick\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc6d95b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.86 ms, sys: 3.62 ms, total: 10.5 ms\n",
      "Wall time: 259 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes, although some have fewer or more.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_2 = \"How many eyes does a spider generally have?\"\n",
    "# Just a rephrasing: but it's the same question,\n",
    "# so it will just take the time to evaluate embeddings\n",
    "llm(SPIDER_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eedbd72",
   "metadata": {},
   "source": [
    "Time for a really new question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fcdae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.1 ms, sys: 1.39 ms, total: 29.5 ms\n",
      "Wall time: 5.56 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nNo, absence of proof is not the same as proof of absence. Absence of proof means that there is no evidence to support a claim, while proof of absence means that there is evidence to support the claim that something does not exist.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LOGIC_QUESTION_FORM_1 = \"Is absence of proof the same as proof of absence?\"\n",
    "# A totally new question\n",
    "llm(LOGIC_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a582be",
   "metadata": {},
   "source": [
    "Going back to the same question as earlier (not literally, though):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1951f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.1 ms, sys: 1.01 ms, total: 16.1 ms\n",
      "Wall time: 246 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes, although some have fewer or more.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_3 = \"How many are the eyes on a spider usually?\"\n",
    "# Trying to catch the cache off-guard :)\n",
    "llm(SPIDER_QUESTION_FORM_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020001df",
   "metadata": {},
   "source": [
    "And again to the logic riddle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61c8b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 ms, sys: 2.66 ms, total: 18.4 ms\n",
      "Wall time: 257 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nNo, absence of proof is not the same as proof of absence. Absence of proof means that there is no evidence to support a claim, while proof of absence means that there is evidence to support the claim that something does not exist.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "LOGIC_QUESTION_FORM_2 = \"Is it true that the absence of a proof equates the proof of an absence?\"\n",
    "# Switching to the other question again\n",
    "llm(LOGIC_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b76c3",
   "metadata": {},
   "source": [
    "## Additional options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e00d1",
   "metadata": {},
   "source": [
    "When creating the semantic cache, you can specify a few other options such as the metric used to calculate the similarity (and, accordingly, a corresponding threshold for accepting a \"cache hit\").\n",
    "\n",
    "Here is an example which uses the L2 (Euclidean) metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ff91966",
   "metadata": {},
   "outputs": [],
   "source": [
    "anotherCassSemanticCache = CassandraSemanticCache(\n",
    "    session=None,\n",
    "    keyspace=None,\n",
    "    embedding=myEmbedding,\n",
    "    table_name=f'semantic_cache_{llmProvider}',\n",
    "    distance_metric='l2',\n",
    "    score_threshold=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae7824",
   "metadata": {},
   "source": [
    "This cache builds on the same database table as the previous one, as can be seen e.g. with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68588ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_entry_id = 77add13036bcaa23c74ebf2ab2c56441-0e4d63bf605cd5f4329128fcbe38762d\n",
      "\n",
      "No, absence of proof is not the same as proof of absence. Absence of proof means that there is no evidence to support a claim, while proof of absence means that there is evidence to support the claim that something does not exist.\n"
     ]
    }
   ],
   "source": [
    "lookup = anotherCassSemanticCache.lookup_with_id_through_llm(\n",
    "    LOGIC_QUESTION_FORM_2,\n",
    "    llm,\n",
    ")\n",
    "if lookup:\n",
    "    cache_entry_id, response = lookup\n",
    "    print(f\"cache_entry_id = {cache_entry_id}\")\n",
    "    # `response` is a List[langchain.schema.output.Generation], so:\n",
    "    print(f\"\\n{response[0].text.strip()}\")\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375cd27",
   "metadata": {},
   "source": [
    "## Caching and Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca45f87f",
   "metadata": {},
   "source": [
    "The `CassandraCache` supports caching within chat-oriented LangChain abstractions such as `ChatOpenAI` as well:\n",
    "\n",
    "_(warning: the following is demonstrated **with OpenAI only** for the time being)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c99f73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ce63ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, supernovae can result in the formation of a black hole. A supernova occurs when a massive star reaches the end of its life and undergoes a catastrophic explosion. The explosion expels most of the star's material into space, while the core collapses under its own gravity.\n",
      "\n",
      "If the core of the star is massive enough, typically more than three times the mass of the Sun, it will collapse further and form a black hole. This collapse is so intense that it creates a region of space with an extremely strong gravitational pull, from which nothing, not even light, can escape. This region is known as a black hole.\n",
      "CPU times: user 25.9 ms, sys: 3.05 ms, total: 29 ms\n",
      "Wall time: 7.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(chat_llm.predict(\"Can supernovae result in a black hole?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70a4724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, supernovae can result in the formation of a black hole. A supernova occurs when a massive star reaches the end of its life and undergoes a catastrophic explosion. The explosion expels most of the star's material into space, while the core collapses under its own gravity.\n",
      "\n",
      "If the core of the star is massive enough, typically more than three times the mass of the Sun, it will collapse further and form a black hole. This collapse is so intense that it creates a region of space with an extremely strong gravitational pull, from which nothing, not even light, can escape. This region is known as a black hole.\n",
      "CPU times: user 15.4 ms, sys: 694 µs, total: 16.1 ms\n",
      "Wall time: 253 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Expect a much faster response:\n",
    "print(chat_llm.predict(\"Is it possible that black holes come from big exploding stars?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aad310",
   "metadata": {},
   "source": [
    "(Actually, every object which inherits from the LangChain `Generation` class can be seamlessly store and retrieved in this cache.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306fe722",
   "metadata": {},
   "source": [
    "## Stale entry control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b39f20",
   "metadata": {},
   "source": [
    "### Time-To-Live (TTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da299a37",
   "metadata": {},
   "source": [
    "You can configure a time-to-live property of the cache, with the effect of automatic eviction of cached entries after a certain time.\n",
    "\n",
    "Setting `langchain.llm_cache` to the following will have the effect that entries vanish in an hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdae9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheWithTTL = CassandraSemanticCache(\n",
    "    session=None,\n",
    "    keyspace=None,\n",
    "    embedding=myEmbedding,\n",
    "    table_name=f'semantic_cache_{llmProvider}',\n",
    "    ttl_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5848837",
   "metadata": {},
   "source": [
    "### Manual cache eviction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79682fb4",
   "metadata": {},
   "source": [
    "Alternatively, you can invalidate individual entries one at a time, just like you saw for the exact-match `CassandraCache` cache.\n",
    "\n",
    "But this is an index based on sentence similarity, so this time the procedure has two steps: **first**, a lookup to find the _id_ of the matching document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fce3834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a1339bc659790da078a4352c05bf422-0e4d63bf605cd5f4329128fcbe38762d\n"
     ]
    }
   ],
   "source": [
    "lookup = cassSemanticCache.lookup_with_id_through_llm(SPIDER_QUESTION_FORM_1, llm)\n",
    "if lookup:\n",
    "    cache_entry_id, response = lookup\n",
    "    print(cache_entry_id)\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43aa54e",
   "metadata": {},
   "source": [
    "_you can see that querying for another form for the \"same\" question will result in the same id:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72bb8b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0a1339bc659790da078a4352c05bf422-0e4d63bf605cd5f4329128fcbe38762d\n"
     ]
    }
   ],
   "source": [
    "lookup2 = cassSemanticCache.lookup_with_id_through_llm(SPIDER_QUESTION_FORM_2, llm)\n",
    "if lookup2:\n",
    "    cache_entry_id2, response2 = lookup2\n",
    "    print(cache_entry_id2)\n",
    "else:\n",
    "    print('No match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeaa1a7",
   "metadata": {},
   "source": [
    "and **second**, the document id is used in the actual cache eviction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a95abd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.delete_by_document_id(cache_entry_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2f83d",
   "metadata": {},
   "source": [
    "As a check, try asking that question again and note the cell execution time (_you can also try re-running the above lookup cell..._):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9b376f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 ms, sys: 947 µs, total: 12.2 ms\n",
      "Wall time: 704 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMost spiders have eight eyes, although some have fewer or more.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c822ac",
   "metadata": {},
   "source": [
    "### Whole-cache deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946aa8bb",
   "metadata": {},
   "source": [
    "Lastly, as you have seen earlier, you can empty the cache entirely with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a4b5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassSemanticCache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What now?\n",
    "\n",
    "This demo is hosted [here](https://cassio.org/frameworks/langchain/semantic-caching-llm-responses/) at cassio.org.\n",
    "\n",
    "Discover the other ways you can integrate \n",
    "Cassandra/Astra DB with your ML/GenAI needs,\n",
    "right **within [your favorite framework](https://cassio.org/frameworks/langchain/about/)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
