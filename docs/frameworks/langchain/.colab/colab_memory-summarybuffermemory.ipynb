{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Buffer Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab-specific setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a Database and get ready to upload the Secure Connect Bundle and supply the Token string\n",
    "(see [Pre-requisites](https://cassio.org/start_here/#vector-database) on cassio.org for details).\n",
    "\n",
    "Likewise, ensure you have the necessary secret for the LLM provider of your choice: you'll be asked to input it shortly\n",
    "(see [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org for details).\n",
    "\n",
    "_Note: this notebook is part of the CassIO documentation. Visit [this page on cassIO.org](https://cassio.org/frameworks/langchain/memory-summarybuffermemory/)._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required dependencies\n",
    "! pip install \\\n",
    "    \"git+https://github.com/hemidactylus/langchain@cassio-full-preview#egg=langchain\" \\\n",
    "    \"cassandra-driver>=3.28.0\" \\\n",
    "    \"cassio==0.0.6\" \\\n",
    "    \"google-cloud-aiplatform>=1.25.0\" \\\n",
    "    \"jupyter>=1.0.0\" \\\n",
    "    \"openai==0.27.7\" \\\n",
    "    \"python-dotenv==1.0.0\" \\\n",
    "    \"tensorflow-cpu==2.12.0\" \\\n",
    "    \"tiktoken==0.4.0\" \\\n",
    "    \"transformers>=4.29.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f44ff",
   "metadata": {},
   "source": [
    "You will likely be asked to \"Restart the Runtime\" at this time, as some dependencies\n",
    "have been upgraded. **Please do restart the runtime now** for a smoother execution from this point onward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your database keyspace name:\n",
    "ASTRA_DB_KEYSPACE = input('Your Astra DB Keyspace name: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your Astra DB token string, the one starting with \"AstraCS:...\"\n",
    "ASTRA_DB_TOKEN_BASED_PASSWORD = input('Your Astra DB Token: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNQ6T_Gjk0Oz"
   },
   "source": [
    "### Astra DB Secure Connect Bundle\n",
    "\n",
    "Please upload the Secure Connect Bundle zipfile to connect to your Astra DB instance.\n",
    "\n",
    "The Secure Connect Bundle is needed to establish a secure connection to the database.\n",
    "Click [here](https://awesome-astra.github.io/docs/pages/astra/download-scb/#c-procedure) for instructions on how to download it from Astra DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your Secure Connect Bundle zipfile:\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "print('Please upload your Secure Connect Bundle')\n",
    "uploaded = files.upload()\n",
    "if uploaded:\n",
    "    astraBundleFileTitle = list(uploaded.keys())[0]\n",
    "    ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab-specific override of helper functions\n",
    "from cassandra.cluster import (\n",
    "    Cluster,\n",
    ")\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "# The \"username\" is the literal string 'token' for this connection mode:\n",
    "ASTRA_DB_TOKEN_BASED_USERNAME = 'token'\n",
    "\n",
    "\n",
    "def getCQLSession(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        cluster = Cluster(\n",
    "            cloud={\n",
    "                \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
    "            },\n",
    "            auth_provider=PlainTextAuthProvider(\n",
    "                ASTRA_DB_TOKEN_BASED_USERNAME,\n",
    "                ASTRA_DB_TOKEN_BASED_PASSWORD,\n",
    "            ),\n",
    "        )\n",
    "        astraSession = cluster.connect()\n",
    "        return astraSession\n",
    "    else:\n",
    "        raise ValueError('Unsupported CQL Session mode')\n",
    "\n",
    "def getCQLKeyspace(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        return ASTRA_DB_KEYSPACE\n",
    "    else:\n",
    "        raise ValueError('Unsupported CQL Session mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXCQ6T_Gjk0Oz"
   },
   "source": [
    "### LLM Provider\n",
    "\n",
    "In the cell below you can choose between **GCP VertexAI** or **OpenAI** for your LLM services.\n",
    "(See [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org for more details).\n",
    "\n",
    "Make sure you set the `llmProvider` variable and supply the corresponding access secrets in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your secret(s) for LLM access:\n",
    "llmProvider = 'OpenAI'  # 'GCP_VertexAI', 'Azure_OpenAI'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llmProvider == 'OpenAI':\n",
    "    apiSecret = input(f'Your secret for LLM provider \"{llmProvider}\": ')\n",
    "    os.environ['OPENAI_API_KEY'] = apiSecret\n",
    "elif llmProvider == 'GCP_VertexAI':\n",
    "    # we need a json file\n",
    "    print(f'Please upload your Service Account JSON for the LLM provider \"{llmProvider}\":')\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    if uploaded:\n",
    "        vertexAIJsonFileTitle = list(uploaded.keys())[0]\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(os.getcwd(), vertexAIJsonFileTitle)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'No file uploaded. Please re-run the cell.'\n",
    "        )\n",
    "elif llmProvider == 'Azure_OpenAI':\n",
    "    # a few parameters must be input\n",
    "    apiSecret = input(f'Your API Key for LLM provider \"{llmProvider}\": ')\n",
    "    os.environ['AZURE_OPENAI_API_KEY'] = apiSecret\n",
    "    apiBase = input('The \"Base URL\" for your models (e.g. \"https://YOUR-RESOURCE-NAME.openai.azure.com\"): ')\n",
    "    os.environ['AZURE_OPENAI_API_BASE'] = apiBase\n",
    "    apiLLMDepl = input('The name of your LLM Deployment: ')\n",
    "    os.environ['AZURE_OPENAI_LLM_DEPLOYMENT'] = apiLLMDepl\n",
    "    apiLLMModel = input('The name of your LLM Model (e.g. \"gpt-4\"): ')\n",
    "    os.environ['AZURE_OPENAI_LLM_MODEL'] = apiLLMModel\n",
    "    apiEmbDepl = input('The name for your Embeddings Deployment: ')\n",
    "    os.environ['AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT'] = apiEmbDepl\n",
    "    apiEmbModel = input('The name of your Embedding Model (e.g. \"text-embedding-ada-002\"): ')\n",
    "    os.environ['AZURE_OPENAI_EMBEDDINGS_MODEL'] = apiEmbModel\n",
    "\n",
    "    # The following is probably not going to change for some time...\n",
    "    os.environ['AZURE_OPENAI_API_VERSION'] = '2023-03-15-preview'\n",
    "else:\n",
    "    raise ValueError('Unknown/unsupported LLM Provider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab preamble completed\n",
    "\n",
    "The following cells constitute the demo notebook proper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafac99",
   "metadata": {},
   "source": [
    "# Summary Buffer Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75a285",
   "metadata": {},
   "source": [
    "The same `CassandraChatMessageHistory` can power a more sophisticated kind of chat memory that automatically maintains a **summary** of the past interactions, so as to provide a memory that virtually extends arbitrarily far back in the past while fitting a finite amount of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35186677",
   "metadata": {},
   "source": [
    "## Notice\n",
    "\n",
    "**this demo is currently not working with the Azure OpenAI LLMs. Please run it with another LLM provider.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd7d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import CassandraChatMessageHistory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec8cd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the DB connection\n",
    "cqlMode = 'astra_db'\n",
    "session = getCQLSession(mode=cqlMode)\n",
    "keyspace = getCQLKeyspace(mode=cqlMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56907ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = CassandraChatMessageHistory(\n",
    "    session_id='summarized-conversation-4321',\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    ttl_seconds = 3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb5dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c02e859",
   "metadata": {},
   "source": [
    "Below is the logic to instantiate the LLM of choice. We choose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68645a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM from OpenAI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# creation of the LLM resources\n",
    "\n",
    "\n",
    "if llmProvider == 'GCP_VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    llm = VertexAI()\n",
    "    print('LLM from VertexAI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    os.environ['OPENAI_API_TYPE'] = 'open_ai'\n",
    "    from langchain.llms import OpenAI\n",
    "    llm = OpenAI()\n",
    "    print('LLM from OpenAI')\n",
    "elif llmProvider == 'Azure_OpenAI':\n",
    "    os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "    os.environ['OPENAI_API_VERSION'] = os.environ['AZURE_OPENAI_API_VERSION']\n",
    "    os.environ['OPENAI_API_BASE'] = os.environ['AZURE_OPENAI_API_BASE']\n",
    "    os.environ['OPENAI_API_KEY'] = os.environ['AZURE_OPENAI_API_KEY']\n",
    "    from langchain.llms import AzureOpenAI\n",
    "    llm = AzureOpenAI(temperature=0, model_name=os.environ['AZURE_OPENAI_LLM_MODEL'],\n",
    "                      engine=os.environ['AZURE_OPENAI_LLM_DEPLOYMENT'])\n",
    "    print('LLM from Azure OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6f715",
   "metadata": {},
   "source": [
    "## Use the memory with summary in a Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5665c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    chat_memory=message_history,\n",
    "    max_token_limit=180,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00b49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryConversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b622450",
   "metadata": {},
   "source": [
    "With the verbose output you can track the summary. It starts empty and once few messages have been exchanged it starts to \"incorporate\" those messages that would have been lost in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea400a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Tell me about William Tell, the hero\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" William Tell was a legendary Swiss hero who lived in the late 13th or early 14th century. He is most known for his feat of shooting an apple off of his son's head with a crossbow. This act was said to be an act of defiance against a tyrannical Austrian ruler, Gessler, who had placed his hat atop a pole in the town square and demanded that all citizens bow to it. Tell's refusal to do so earned him his place in Swiss folklore.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryConversation.predict(input='Tell me about William Tell, the hero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8c60d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Tell me about William Tell, the hero\n",
      "AI:  William Tell was a legendary Swiss hero who lived in the late 13th or early 14th century. He is most known for his feat of shooting an apple off of his son's head with a crossbow. This act was said to be an act of defiance against a tyrannical Austrian ruler, Gessler, who had placed his hat atop a pole in the town square and demanded that all citizens bow to it. Tell's refusal to do so earned him his place in Swiss folklore.\n",
      "Human: Any relation to the apple from Cupertino?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I do not know.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryConversation.predict(\n",
    "    input=\"Any relation to the apple from Cupertino?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6312b1fe",
   "metadata": {},
   "source": [
    "No messages have slided past the \"recent window\" yet -- correspondingly, the summary buffer is still empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bacf5598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.moving_summary_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8858b4c",
   "metadata": {},
   "source": [
    "Continue the conversation - you might start to see the summary of past exchanges making its way to the final prompt for the LLM in the form of a \"System\" contribution to the _Current conversation_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a392a4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Tell me about William Tell, the hero\n",
      "AI:  William Tell was a legendary Swiss hero who lived in the late 13th or early 14th century. He is most known for his feat of shooting an apple off of his son's head with a crossbow. This act was said to be an act of defiance against a tyrannical Austrian ruler, Gessler, who had placed his hat atop a pole in the town square and demanded that all citizens bow to it. Tell's refusal to do so earned him his place in Swiss folklore.\n",
      "Human: Any relation to the apple from Cupertino?\n",
      "AI: I do not know.\n",
      "Human: I heard the two apples may be in fact the very same fruit.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That seems highly unlikely given that William Tell's feat is said to have happened in the 13th or 14th century, while the Apple from Cupertino was founded in the late 20th century.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryConversation.predict(\n",
    "    input=\"I heard the two apples may be in fact the very same fruit.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f3c5e",
   "metadata": {},
   "source": [
    "This is where the current summary buffer is stored\n",
    "\n",
    "_(Note: depending on the length of the messages so far, at this point you might or might not see an empty summary. In any case, keep reading and it will doubtless start to be nonempty.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd16bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe human asks the AI to tell them about William Tell, the hero.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.moving_summary_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c7959",
   "metadata": {},
   "source": [
    "The whole conversation is still in the \"recent\" window. Below is what will start to happen in the `ConversationSummaryBufferMemory` at the next chat message: starting from the message list, and the summary so far (if any), the LLM is asked to come up with a new summary which will overwrite the current one, stored in `memory.moving_summary_buffer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d28243d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human asked the AI to tell them about William Tell, the hero. The AI explained that Tell was a legendary Swiss hero who was known for shooting an apple off of his son's head with a crossbow to defy a tyrannical Austrian ruler, Gessler. The human then asked if there was any relation to the Apple from Cupertino, to which the AI replied that it was unlikely given that Tell's feat happened centuries before the company was founded.\n"
     ]
    }
   ],
   "source": [
    "print(memory.predict_new_summary(\n",
    "    memory.chat_memory.messages,\n",
    "    memory.moving_summary_buffer,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "939cb390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human asks the AI to tell them about William Tell, the hero.\n",
      "Human: Tell me about William Tell, the hero\n",
      "AI:  William Tell was a legendary Swiss hero who lived in the late 13th or early 14th century. He is most known for his feat of shooting an apple off of his son's head with a crossbow. This act was said to be an act of defiance against a tyrannical Austrian ruler, Gessler, who had placed his hat atop a pole in the town square and demanded that all citizens bow to it. Tell's refusal to do so earned him his place in Swiss folklore.\n",
      "Human: Any relation to the apple from Cupertino?\n",
      "AI: I do not know.\n",
      "Human: I heard the two apples may be in fact the very same fruit.\n",
      "AI:  That seems highly unlikely given that William Tell's feat is said to have happened in the 13th or 14th century, while the Apple from Cupertino was founded in the late 20th century.\n",
      "Human: Ok, enough with apples. Let's talk starfish now.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Sure! Starfish, or sea stars, are a group of echinoderms found in all of the world's oceans. They have five arms and a central disc, and they range in size from a few centimeters to over a meter in diameter. Starfish are found in a variety of habitats, from shallow tidal pools to deep sea floors. They are carnivorous predators and feed on mollusks, crustaceans, and other small organisms.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryConversation.predict(\n",
    "    input='Ok, enough with apples. Let\\'s talk starfish now.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "530c600c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe human asks the AI to tell them about William Tell, the hero. The AI explains that William Tell was a legendary Swiss hero from the late 13th or early 14th century, who is most known for shooting an apple off his son's head with a crossbow. This act was an act of defiance against an Austrian ruler, Gessler, who had placed his hat atop a pole in the town square and demanded that all citizens bow to it. The human then jokes about any relation to the apple from Cupertino.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.moving_summary_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23fcbe",
   "metadata": {},
   "source": [
    "Now the chat history has presumably reached a certain threshold and there is a summary. In the verbose output below, you will see it appearing in the prompt as a \"System\" role in the conversation recap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0a3088e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human asks the AI to tell them about William Tell, the hero. The AI explains that William Tell was a legendary Swiss hero from the late 13th or early 14th century, who is most known for shooting an apple off his son's head with a crossbow. This act was an act of defiance against an Austrian ruler, Gessler, who had placed his hat atop a pole in the town square and demanded that all citizens bow to it. The human then jokes about any relation to the apple from Cupertino.\n",
      "Human: Tell me about William Tell, the hero\n",
      "AI:  William Tell was a legendary Swiss hero who lived in the late 13th or early 14th century. He is most known for his feat of shooting an apple off of his son's head with a crossbow. This act was said to be an act of defiance against a tyrannical Austrian ruler, Gessler, who had placed his hat atop a pole in the town square and demanded that all citizens bow to it. Tell's refusal to do so earned him his place in Swiss folklore.\n",
      "Human: Any relation to the apple from Cupertino?\n",
      "AI: I do not know.\n",
      "Human: I heard the two apples may be in fact the very same fruit.\n",
      "AI:  That seems highly unlikely given that William Tell's feat is said to have happened in the 13th or 14th century, while the Apple from Cupertino was founded in the late 20th century.\n",
      "Human: Ok, enough with apples. Let's talk starfish now.\n",
      "AI:  Sure! Starfish, or sea stars, are a group of echinoderms found in all of the world's oceans. They have five arms and a central disc, and they range in size from a few centimeters to over a meter in diameter. Starfish are found in a variety of habitats, from shallow tidal pools to deep sea floors. They are carnivorous predators and feed on mollusks, crustaceans, and other small organisms.\n",
      "Human: I hear that some of those destroy marine ecosystems.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Yes, that is true. Some species of starfish, notably the crown-of-thorns starfish, can be invasive and destructive to coral reef ecosystems. The starfish feed on the coral, often causing catastrophic damage to the reef.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryConversation.predict(\n",
    "    input='I hear that some of those destroy marine ecosystems.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51503316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe human asks the AI to tell them about William Tell, the hero. The AI explains that William Tell was a legendary Swiss hero from the late 13th or early 14th century, who is most known for shooting an apple off his son's head with a crossbow. This act was an act of defiance against an Austrian ruler, Gessler, who had placed his hat atop a pole in the town square and demanded that all citizens bow to it. The human then jokes about any relation to the apple from Cupertino, to which the AI responds that it is highly unlikely given the timeline of their respective histories.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.moving_summary_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf8c078",
   "metadata": {},
   "source": [
    "From this point on, the summary will not grow indefinitely in size, but will rather try to cover a longer and longer conversation in a fixed amount of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0154545f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human asks the AI to tell them about William Tell, the hero. The AI explains that William Tell was a legendary Swiss hero from the late 13th or early 14th century, who is most known for shooting an apple off his son's head with a crossbow as an act of defiance against a tyrannical Austrian ruler. The human then jokes about any relation to the apple from Cupertino, to which the AI responds that it is highly unlikely given the timeline of their respective histories. The human then shifts the conversation to starfish, to which the AI explains that they are a group of echinoderms found in all the world's oceans. The AI also mentions that some species of starfish, such as the crown-of-thorns starfish, can be invasive and destructive to coral reef ecosystems.\n"
     ]
    }
   ],
   "source": [
    "print(memory.predict_new_summary(\n",
    "    memory.chat_memory.messages,\n",
    "    memory.moving_summary_buffer,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What now?\n",
    "\n",
    "This demo is hosted [here](https://cassio.org/frameworks/langchain/memory-summarybuffermemory/) at cassio.org.\n",
    "\n",
    "Discover the other ways you can integrate \n",
    "Cassandra/Astra DB with your ML/GenAI needs,\n",
    "right **within [your favorite framework](https://cassio.org/frameworks/langchain/about/)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
