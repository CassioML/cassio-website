{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorStore-backed memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab-specific setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a Database, a Secure Connect Bundle and a Database Token. See [DB Setup](https://cassio.org/db_setup/) on cassio.org for details, **paying attention to the notes for Colab users**.\n",
    "Get ready to upload the Bundle and supply the Token string.\n",
    "\n",
    "Likewise, ensure you have the necessary secret for the LLM provider of your choice: you'll be asked to input it shortly. See [API Setup](https://cassio.org/api_setup/) on cassio.org for details, **paying attention to the notes for Colab users**.\n",
    "\n",
    "_Note: this colab is autogenerated from a [regular Jupyter notebook](https://cassio.org/frameworks/langchain/memory-vectorstore/) hosted on cassio.org. Run all cells in this section to complete the setup before moving on to the demo content proper._\n",
    "\n",
    "_Note: you can work with your own Cassandra cluster instead of Astra DB, provided it is reachable from the cloud: check the commented code below for the case `cqlMode=\"local\"` and consult [cassio.org](https://cassio.org/frameworks/langchain/setup/#database-choice) for more details._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required dependencies\n",
    "! pip install \\\n",
    "    \"git+https://github.com/hemidactylus/langchain@cassio#egg=langchain\" \\\n",
    "    \"cassandra-driver>=3.28.0\" \\\n",
    "    \"cassio>=0.0.4\" \\\n",
    "    \"google-cloud-aiplatform>=1.25.0\" \\\n",
    "    \"jupyter>=1.0.0\" \\\n",
    "    \"openai==0.27.7\" \\\n",
    "    \"python-dotenv==1.0.0\" \\\n",
    "    \"tensorflow-cpu==2.12.0\" \\\n",
    "    \"tiktoken==0.4.0\" \\\n",
    "    \"transformers>=4.29.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your database keyspace name:\n",
    "ASTRA_DB_KEYSPACE = input('Your Astra DB Keyspace name: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your Astra DB token string, the one starting with \"AstraCS:...\"\n",
    "ASTRA_DB_APPLICATION_TOKEN = input('Your Astra DB Token: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNQ6T_Gjk0Oz"
   },
   "source": [
    "### Astra DB Secure Connect Bundle\n",
    "\n",
    "Please upload the Secure Connect Bundle zipfile to connect to your Astra DB instance.\n",
    "\n",
    "The Secure Connect Bundle is needed to establish a secure connection to the database.\n",
    "Click [here](https://awesome-astra.github.io/docs/pages/astra/download-scb/#c-procedure) for instructions on how to download it from Astra DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your Secure Connect Bundle zipfile:\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "print('Please upload your Secure Connect Bundle')\n",
    "uploaded = files.upload()\n",
    "if uploaded:\n",
    "    astraBundleFileTitle = list(uploaded.keys())[0]\n",
    "    ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab-specific override of helper functions\n",
    "from cassandra.cluster import (\n",
    "    Cluster,\n",
    ")\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "ASTRA_DB_CLIENT_ID = 'token'\n",
    "\n",
    "\n",
    "def getCQLSession(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        cluster = Cluster(\n",
    "            cloud={\n",
    "                \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
    "            },\n",
    "            auth_provider=PlainTextAuthProvider(\n",
    "                ASTRA_DB_CLIENT_ID,\n",
    "                ASTRA_DB_APPLICATION_TOKEN,\n",
    "            ),\n",
    "        )\n",
    "        astraSession = cluster.connect()\n",
    "        return astraSession\n",
    "    # elif mode == 'local':\n",
    "    #     cluster = Cluster(\n",
    "    #         ['192.168.0.1', '192.168.0.2'],\n",
    "    #         auth_provider=PlainTextAuthProvider(\n",
    "    #             \"username\",\n",
    "    #             \"password!\",\n",
    "    #         ),\n",
    "    #     )\n",
    "    #     # See https://docs.datastax.com/en/developer/python-driver/latest/getting_started/#connecting-to-cassandra for more options\n",
    "    #     localSession = cluster.connect()\n",
    "    #     return localSession\n",
    "    else:\n",
    "        raise ValueError('Unknown CQL Session mode')\n",
    "\n",
    "def getCQLKeyspace(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        return ASTRA_DB_KEYSPACE\n",
    "    # elif mode == 'local':\n",
    "    #     return <NAME_OF_YOUR_LOCAL_KEYSPACE>\n",
    "    else:\n",
    "        raise ValueError('Unknown CQL Session mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXCQ6T_Gjk0Oz"
   },
   "source": [
    "### LLM Provider\n",
    "\n",
    "In the cell below you can choose between **GCP VertexAI** or **OpenAI** for your LLM services.\n",
    "Make sure you set the `llmProvider` variable and supply the corresponding access secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your secret(s) for LLM access (key names must match `providerValidator` in `llm_choice`)\n",
    "llmProvider = 'OpenAI'  # 'GCP_VertexAI'\n",
    "if llmProvider == 'OpenAI':\n",
    "    apiSecret = input(f'Your secret for LLM provider \"{llmProvider}\": ')\n",
    "    os.environ['OPENAI_API_KEY'] = apiSecret\n",
    "elif llmProvider == 'GCP_VertexAI':\n",
    "    # we need a json file\n",
    "    print(f'Please upload your Service Account JSON for the LLM provider \"{llmProvider}\":')\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    if uploaded:\n",
    "        vertexAIJsonFileTitle = list(uploaded.keys())[0]\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(os.getcwd(), vertexAIJsonFileTitle)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'No file uploaded. Please re-run the cell.'\n",
    "        )\n",
    "else:\n",
    "    raise ValueError('Unknown/unsupported LLM Provider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab preamble completed\n",
    "\n",
    "The following cells constitute the demo notebook proper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24440ad",
   "metadata": {},
   "source": [
    "# VectorStore-backed memory\n",
    "\n",
    "The support for Cassandra vector store, available in LangChain, enables another interesting use case, namely a chat memory buffer that injects the most relevant past exchanges into the prompt, instead of the most recent (as most other memories do). This enables retrieval of related context _arbitrarily far back in the chat history_.\n",
    "\n",
    "All you need is to instantiate a `Cassandra` vector store and wrap it in a `VectorStoreRetrieverMemory` type of memory, provided by LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d089d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5eca9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.cassandra import Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d19647",
   "metadata": {},
   "source": [
    "As usual, a database connection is needed to access Cassandra. The following assumes\n",
    "that a _vector-search-capable Astra DB instance_ is available. Adjust as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf9f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the DB connection\n",
    "cqlMode = 'astra_db' # alternatively, 'local' ... if you do have a Cassandra cluster to use, that is\n",
    "session = getCQLSession(mode=cqlMode)\n",
    "keyspace = getCQLKeyspace(mode=cqlMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b5aae8",
   "metadata": {},
   "source": [
    "Both an LLM and an embedding function are required.\n",
    "\n",
    "Below is the logic to instantiate the LLM and embeddings of choice. We choose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14603440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM+embeddings from OpenAI\n"
     ]
    }
   ],
   "source": [
    "# creation of the LLM resources\n",
    "\n",
    "\n",
    "if llmProvider == 'GCP_VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    from langchain.embeddings import VertexAIEmbeddings\n",
    "    llm = VertexAI()\n",
    "    myEmbedding = VertexAIEmbeddings()\n",
    "    print('LLM+embeddings from VertexAI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = OpenAI(temperature=0)\n",
    "    myEmbedding = OpenAIEmbeddings()\n",
    "    print('LLM+embeddings from OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8393b",
   "metadata": {},
   "source": [
    "## Create the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b2b44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'vstore_memory_' + llmProvider\n",
    "cassVStore = Cassandra(\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    table_name=table_name,\n",
    "    embedding=myEmbedding,\n",
    ")\n",
    "\n",
    "# just in case this demo runs multiple times\n",
    "cassVStore.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2843410",
   "metadata": {},
   "source": [
    "## Create the retriever and the memory\n",
    "\n",
    "From the vector store a \"retriever\" is spawned. You'll keep the number of items to fetch intentionally very small for demonstration purposes.\n",
    "\n",
    "Next, the retriever is wrapped in a `VectorStoreRetrieverMemory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62bdb85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = cassVStore.as_retriever(search_kwargs={'k': 3})\n",
    "semanticMemory = VectorStoreRetrieverMemory(retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a4cd9a",
   "metadata": {},
   "source": [
    "Create a fake \"past conversation\". Note how the topic of the discussion wanders to fixing one's PC in the last few exchanges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e723384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pastExchanges = [\n",
    "    (\n",
    "        {\"input\": \"Hello, what is the biggest mammal?\"},\n",
    "        {\"output\": \"The blue whale.\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"input\": \"... I cannot swim. Actually I hate swimming!\"},\n",
    "        {\"output\": \"I see.\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"input\": \"I like mountains and beech forests.\"},\n",
    "        {\"output\": \"That's good to know.\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"input\": \"Yes, too much water makes me uneasy.\"},\n",
    "        {\"output\": \"Ah, how come?.\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"input\": \"I guess I am just not a seaside person\"},\n",
    "        {\"output\": \"I see. How may I help you?\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"input\": \"I need help installing this driver\"},\n",
    "        {\"output\": \"First download the right version for your operating system.\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"input\": \"Good grief ... my keyboard does not work anymore!\"},\n",
    "        {\"output\": \"Try plugging it in your PC first.\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b9f4b",
   "metadata": {},
   "source": [
    "Insert these exchanges into the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c114f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exI, exO in pastExchanges:\n",
    "    semanticMemory.save_context(exI, exO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4c1c59",
   "metadata": {},
   "source": [
    "Given a conversation input, the `load_memory_variables` performs a semantic search and comes up with relevant items from the memory, regardless of their order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e54892",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Can you suggest me a sport to try?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a646c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ... I cannot swim. Actually I hate swimming!\n",
      "output: I see.\n",
      "input: I guess I am just not a seaside person\n",
      "output: I see. How may I help you?\n",
      "input: I like mountains and beech forests.\n",
      "output: That's good to know.\n"
     ]
    }
   ],
   "source": [
    "print(semanticMemory.load_memory_variables({\"prompt\": QUESTION})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3689b2",
   "metadata": {},
   "source": [
    "## Usage in a conversation chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95178b81",
   "metadata": {},
   "source": [
    "This semantic memory element can be used within a full conversation chain.\n",
    "\n",
    "In the following you'll create a custom prompt and a `ConversationChain` out of it, attaching the latter to the vector-store-powered memory seen above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a946b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "semanticMemoryTemplateString = \"\"\"The following is a between a human and a helpful AI.\n",
    "The AI is talkative and provides lots of specific details from its context.\n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "The AI can use information from parts of the previous conversation (only if they are relevant):\n",
    "{history}\n",
    "\n",
    "Current conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "memoryPrompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=semanticMemoryTemplateString\n",
    ")\n",
    "\n",
    "conversationWithVectorRetrieval = ConversationChain(\n",
    "    llm=llm, \n",
    "    prompt=memoryPrompt,\n",
    "    memory=semanticMemory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e6d4a",
   "metadata": {},
   "source": [
    "Run the chain with the sports question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ff9fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a between a human and a helpful AI.\n",
      "The AI is talkative and provides lots of specific details from its context.\n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "The AI can use information from parts of the previous conversation (only if they are relevant):\n",
      "input: ... I cannot swim. Actually I hate swimming!\n",
      "output: I see.\n",
      "input: I guess I am just not a seaside person\n",
      "output: I see. How may I help you?\n",
      "input: I like mountains and beech forests.\n",
      "output: That's good to know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Can you suggest me a sport to try?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Sure, I can suggest some sports for you to try. Depending on your preferences, you could try hiking, running, biking, or even rock climbing. Do any of these sound interesting to you?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversationWithVectorRetrieval.predict(input=QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20059880",
   "metadata": {},
   "source": [
    "Notice how new exchanges are automatically added to the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "356fd41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a between a human and a helpful AI.\n",
      "The AI is talkative and provides lots of specific details from its context.\n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "The AI can use information from parts of the previous conversation (only if they are relevant):\n",
      "input: I like mountains and beech forests.\n",
      "output: That's good to know.\n",
      "input: Can you suggest me a sport to try?\n",
      "response:  Sure, I can suggest some sports for you to try. Depending on your preferences, you could try hiking, running, biking, or even rock climbing. Do any of these sound interesting to you?\n",
      "input: ... I cannot swim. Actually I hate swimming!\n",
      "output: I see.\n",
      "\n",
      "Current conversation:\n",
      "Human: Would I like a swim in a mountain lake?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That depends on your preferences. If you don't like swimming, then a swim in a mountain lake may not be the best activity for you. However, if you enjoy the outdoors and the scenery of a mountain lake, then it could be a great experience.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversationWithVectorRetrieval.predict(input=\"Would I like a swim in a mountain lake?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4bfad9",
   "metadata": {},
   "source": [
    "... so that now the most relevant items for the same question are changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce0a74be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='input: Can you suggest me a sport to try?\\nresponse:  Sure, I can suggest some sports for you to try. Depending on your preferences, you could try hiking, running, biking, or even rock climbing. Do any of these sound interesting to you?', metadata={}),\n",
       " Document(page_content=\"input: Would I like a swim in a mountain lake?\\nresponse:  That depends on your preferences. If you don't like swimming, then a swim in a mountain lake may not be the best activity for you. However, if you enjoy the outdoors and the scenery of a mountain lake, then it could be a great experience.\", metadata={}),\n",
       " Document(page_content='input: ... I cannot swim. Actually I hate swimming!\\noutput: I see.', metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semanticMemory.retriever.get_relevant_documents(QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa99314",
   "metadata": {},
   "source": [
    "## A counterexample\n",
    "\n",
    "What would happen with a simpler memory element, which simply retrieves a certain number of most recent interactions?\n",
    "\n",
    "Create and populate an instance of LangChain's `ConversationTokenBufferMemory`, limiting it to a maximum token length of 80 (roughly equivalent to the 3 fragments set for the `semanticMemory` object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4306aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e31863f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseHistory = ChatMessageHistory()\n",
    "\n",
    "recencyBufferMemory = ConversationTokenBufferMemory(\n",
    "    chat_memory=baseHistory,\n",
    "    max_token_limit=80,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exI, exO in pastExchanges:\n",
    "    recencyBufferMemory.save_context(exI, exO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7eb975",
   "metadata": {},
   "source": [
    "Time to ask the same sports question. This is what will get injected into the prompt this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0b9564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(recencyBufferMemory.load_memory_variables({\"prompt\": QUESTION})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18863525",
   "metadata": {},
   "source": [
    "... and this is the (rather generic) answer you'd get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdc52f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a between a human and a helpful AI.\n",
      "The AI is talkative and provides lots of specific details from its context.\n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "The AI can use information from parts of the previous conversation (only if they are relevant):\n",
      "\n",
      "\n",
      "Current conversation:\n",
      "Human: Can you suggest me a sport to try?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Sure! Have you ever tried tennis? It's a great sport that can be played both indoors and outdoors. It's a great way to get some exercise and have fun at the same time. Plus, it's a great way to meet new people and make new friends.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversationWithRecencyRetrieval = ConversationChain(\n",
    "    llm=llm, \n",
    "    prompt=memoryPrompt,\n",
    "    memory=recencyBufferMemory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversationWithRecencyRetrieval.predict(input=QUESTION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
