{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorStore/QA, MMR support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab-specific setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a Database and get ready to upload the Secure Connect Bundle and supply the Token string\n",
    "(see [Pre-requisites](https://cassio.org/start_here/#vector-database) on cassio.org for details).\n",
    "\n",
    "Likewise, ensure you have the necessary secret for the LLM provider of your choice: you'll be asked to input it shortly\n",
    "(see [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org for details).\n",
    "\n",
    "_Note: this notebook is part of the CassIO documentation. Visit [this page on cassIO.org](https://cassio.org/frameworks/langchain/qa-maximal-marginal-relevance/)._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required dependencies\n",
    "! pip install \\\n",
    "    \"git+https://github.com/hemidactylus/langchain@cassio-full-preview#egg=langchain\" \\\n",
    "    \"cassandra-driver>=3.28.0\" \\\n",
    "    \"cassio==0.0.6\" \\\n",
    "    \"google-cloud-aiplatform>=1.25.0\" \\\n",
    "    \"jupyter>=1.0.0\" \\\n",
    "    \"openai==0.27.7\" \\\n",
    "    \"python-dotenv==1.0.0\" \\\n",
    "    \"tensorflow-cpu==2.12.0\" \\\n",
    "    \"tiktoken==0.4.0\" \\\n",
    "    \"transformers>=4.29.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f44ff",
   "metadata": {},
   "source": [
    "You will likely be asked to \"Restart the Runtime\" at this time, as some dependencies\n",
    "have been upgraded. **Please do restart the runtime now** for a smoother execution from this point onward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your database keyspace name:\n",
    "ASTRA_DB_KEYSPACE = input('Your Astra DB Keyspace name: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your Astra DB token string, the one starting with \"AstraCS:...\"\n",
    "ASTRA_DB_TOKEN_BASED_PASSWORD = input('Your Astra DB Token: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNQ6T_Gjk0Oz"
   },
   "source": [
    "### Astra DB Secure Connect Bundle\n",
    "\n",
    "Please upload the Secure Connect Bundle zipfile to connect to your Astra DB instance.\n",
    "\n",
    "The Secure Connect Bundle is needed to establish a secure connection to the database.\n",
    "Click [here](https://awesome-astra.github.io/docs/pages/astra/download-scb/#c-procedure) for instructions on how to download it from Astra DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your Secure Connect Bundle zipfile:\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "print('Please upload your Secure Connect Bundle')\n",
    "uploaded = files.upload()\n",
    "if uploaded:\n",
    "    astraBundleFileTitle = list(uploaded.keys())[0]\n",
    "    ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab-specific override of helper functions\n",
    "from cassandra.cluster import (\n",
    "    Cluster,\n",
    ")\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "# The \"username\" is the literal string 'token' for this connection mode:\n",
    "ASTRA_DB_TOKEN_BASED_USERNAME = 'token'\n",
    "\n",
    "\n",
    "def getCQLSession(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        cluster = Cluster(\n",
    "            cloud={\n",
    "                \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
    "            },\n",
    "            auth_provider=PlainTextAuthProvider(\n",
    "                ASTRA_DB_TOKEN_BASED_USERNAME,\n",
    "                ASTRA_DB_TOKEN_BASED_PASSWORD,\n",
    "            ),\n",
    "        )\n",
    "        astraSession = cluster.connect()\n",
    "        return astraSession\n",
    "    else:\n",
    "        raise ValueError('Unsupported CQL Session mode')\n",
    "\n",
    "def getCQLKeyspace(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        return ASTRA_DB_KEYSPACE\n",
    "    else:\n",
    "        raise ValueError('Unsupported CQL Session mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXCQ6T_Gjk0Oz"
   },
   "source": [
    "### LLM Provider\n",
    "\n",
    "In the cell below you can choose between **GCP VertexAI** or **OpenAI** for your LLM services.\n",
    "(See [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org for more details).\n",
    "\n",
    "Make sure you set the `llmProvider` variable and supply the corresponding access secrets in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your secret(s) for LLM access:\n",
    "llmProvider = 'OpenAI'  # 'GCP_VertexAI'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llmProvider == 'OpenAI':\n",
    "    apiSecret = input(f'Your secret for LLM provider \"{llmProvider}\": ')\n",
    "    os.environ['OPENAI_API_KEY'] = apiSecret\n",
    "elif llmProvider == 'GCP_VertexAI':\n",
    "    # we need a json file\n",
    "    print(f'Please upload your Service Account JSON for the LLM provider \"{llmProvider}\":')\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    if uploaded:\n",
    "        vertexAIJsonFileTitle = list(uploaded.keys())[0]\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(os.getcwd(), vertexAIJsonFileTitle)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'No file uploaded. Please re-run the cell.'\n",
    "        )\n",
    "else:\n",
    "    raise ValueError('Unknown/unsupported LLM Provider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab preamble completed\n",
    "\n",
    "The following cells constitute the demo notebook proper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e19ad",
   "metadata": {},
   "source": [
    "# VectorStore/QA, MMR support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0778d5f6",
   "metadata": {},
   "source": [
    "_**NOTE:** this uses Cassandra's \"Vector Search\" capability.\n",
    "Make sure you are connecting to a vector-enabled database for this demo._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fec23",
   "metadata": {},
   "source": [
    "Cassandra's `VectorStore` allows for Vector Search with the **Maximal Marginal Relevance (MMR)** algorithm.\n",
    "\n",
    "This is a search criterion that instead of just selecting the _k_ stored documents most relevant to the provided query, first identifies a larger pool of relevant results, and then singles out _k_ of them so that they carry as diverse information between them as possible.\n",
    "\n",
    "In this way, when the stored text fragments are likely to be redundant, you can optimize token usage and help the models give more comprehensive answers.\n",
    "\n",
    "_This is very useful, for instance, if you are building a QA chatbot on past Support chat recorded interactions._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47170e52",
   "metadata": {},
   "source": [
    "First prepare a connection to a vector-search-capable Cassandra and initialize the required LLM and embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7672da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.vectorstores import Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "777b8c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the DB connection\n",
    "cqlMode = 'astra_db'\n",
    "session = getCQLSession(mode=cqlMode)\n",
    "keyspace = getCQLKeyspace(mode=cqlMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df3f86",
   "metadata": {},
   "source": [
    "Below is the logic to instantiate the LLM and embeddings of choice. We choose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63d3c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM+embeddings from OpenAI\n"
     ]
    }
   ],
   "source": [
    "# creation of the LLM resources\n",
    "\n",
    "\n",
    "if llmProvider == 'GCP_VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    from langchain.embeddings import VertexAIEmbeddings\n",
    "    llm = VertexAI(temperature=0)\n",
    "    myEmbedding = VertexAIEmbeddings()\n",
    "    print('LLM+embeddings from VertexAI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = OpenAI(temperature=0)\n",
    "    myEmbedding = OpenAIEmbeddings()\n",
    "    print('LLM+embeddings from OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a0110b",
   "metadata": {},
   "source": [
    "## Create the store\n",
    "\n",
    "Create a (Cassandra-backed) `VectorStore` and the corresponding LangChain `VectorStoreIndexWrapper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e0fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "myCassandraVStore = Cassandra(\n",
    "    embedding=myEmbedding,\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    table_name='vs_test2_' + llmProvider,\n",
    ")\n",
    "index = VectorStoreIndexWrapper(vectorstore=myCassandraVStore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0141b35",
   "metadata": {},
   "source": [
    "This command simply resets the store in case you want to run this demo repeatedly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d88fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myCassandraVStore.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f19c2",
   "metadata": {},
   "source": [
    "## Populate the index\n",
    "\n",
    "\n",
    "Notice that the first four sentences express the same concept, while the **fifth** adds a new detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53c343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_SENTENCE_0 =     ('The frogs and the toads were meeting in the night '\n",
    "                       'for a party under the moon.')\n",
    "\n",
    "BASE_SENTENCE_1 =     ('There was a party under the moon, that all toads, '\n",
    "                       'with the frogs, decided to throw that night.')\n",
    "\n",
    "BASE_SENTENCE_2 =     ('And the frogs and the toads said: \"Let us have a party '\n",
    "                       'tonight, as the moon is shining\".')\n",
    "\n",
    "BASE_SENTENCE_3 =     ('I remember that night... toads, along with frogs, '\n",
    "                       'were all busy planning a moonlit celebration.')\n",
    "\n",
    "DIFFERENT_SENTENCE =  ('For the party, frogs and toads set a rule: '\n",
    "                       'everyone was to wear a purple hat.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40ae400",
   "metadata": {},
   "source": [
    "Insert the three into the index, specifying \"sources\" while you're at it (it will be useful later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7a3935c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d86ebe8bf2f6fa27ff01db7e3c4a21ab',\n",
       " 'be4fffdf596f08c1d3f4d9effc2f327e',\n",
       " 'd25517400eac2ff0eb4c9c2b38d5e7db',\n",
       " '7bb2aec568c5577c107a403f2fb1a64e',\n",
       " '4654a61925e397ea5f097019b2fa56d2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCassandraVStore.add_texts(\n",
    "    [\n",
    "        BASE_SENTENCE_0,\n",
    "        BASE_SENTENCE_1,\n",
    "        BASE_SENTENCE_2,\n",
    "        BASE_SENTENCE_3,\n",
    "        DIFFERENT_SENTENCE,\n",
    "    ],\n",
    "    metadatas=[\n",
    "        {'source': 'Barney\\'s story at the pub'},\n",
    "        {'source': 'Barney\\'s story at the pub'},\n",
    "        {'source': 'Barney\\'s story at the pub'},\n",
    "        {'source': 'Barney\\'s story at the pub'},\n",
    "        {'source': 'The chronicles at the village library'},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0657f",
   "metadata": {},
   "source": [
    "## Query the store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f36c77",
   "metadata": {},
   "source": [
    "Here is the question you'll use to query the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a522924",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = 'Tell me about the party that night.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19929621",
   "metadata": {},
   "source": [
    "### Query with \"similarity\" search type\n",
    "\n",
    "If you ask for two matches, you will get the two documents most related to the question. But in this case this is something of a waste of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "050b7808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0]: \"There was a party under the moon, that all toads, with the frogs, decided to throw that night.\"\n",
      "[ 1]: \"I remember that night... toads, along with frogs, were all busy planning a moonlit celebration.\"\n"
     ]
    }
   ],
   "source": [
    "matchesSim = myCassandraVStore.search(QUESTION, search_type='similarity', k=2)\n",
    "for i, doc in enumerate(matchesSim):\n",
    "    print(f'[{i:2}]: \"{doc.page_content}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979451c6",
   "metadata": {},
   "source": [
    "### Query with MMR\n",
    "\n",
    "Now, here's what happens with the MMR search type.\n",
    "\n",
    "_(Not shown here: you can tune the size of the results pool for the first step of the algorithm.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3fa9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0]: \"There was a party under the moon, that all toads, with the frogs, decided to throw that night.\"\n",
      "[ 1]: \"For the party, frogs and toads set a rule: everyone was to wear a purple hat.\"\n"
     ]
    }
   ],
   "source": [
    "matchesMMR = myCassandraVStore.search(QUESTION, search_type='mmr', k=2)\n",
    "for i, doc in enumerate(matchesMMR):\n",
    "    print(f'[{i:2}]: \"{doc.page_content}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2614d119",
   "metadata": {},
   "source": [
    "## Query the index\n",
    "\n",
    "Currently, LangChain's higher \"index\" abstraction does not allow to specify the search type, nor the number of matches subsequently used in creating the answer. So, by running this command you get an answer, all right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b0fad86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The frogs and toads were having a party under the moon that night. They were busy planning and celebrating together.\n"
     ]
    }
   ],
   "source": [
    "# (implicitly) by similarity\n",
    "print(index.query(QUESTION, llm=llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65e207",
   "metadata": {},
   "source": [
    "You can request the question-answering process to provide references (as long as you annotated all input documents with a `source` metadata field):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb031f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic chain (implicitly by similarity):\n",
      "  ANSWER : The frogs and toads were planning a party under the moon that night.\n",
      "  SOURCES: Barney's story at the pub\n"
     ]
    }
   ],
   "source": [
    "responseSrc = index.query_with_sources(QUESTION, llm=llm)\n",
    "print('Automatic chain (implicitly by similarity):')\n",
    "print(f'  ANSWER : {responseSrc[\"answer\"].strip()}')\n",
    "print(f'  SOURCES: {responseSrc[\"sources\"].strip()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd2149c",
   "metadata": {},
   "source": [
    "Here the default is to fetch _four_ documents ... so that the only other text actually carrying additional information is left out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d51f2c",
   "metadata": {},
   "source": [
    "### The QA Process behind the scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd6fa3",
   "metadata": {},
   "source": [
    "In order to exploit the MMR search in end-to-end question-answering pipelines, you need to recreate and manually tweak the steps behind the `query` or `query_with_sources` methods. This takes just a few lines.\n",
    "\n",
    "First you need a few additional modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42e6822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed211a1",
   "metadata": {},
   "source": [
    "You are ready to run two QA chains, identical in all respects (especially in the number of results to fetch, two), except the `search_type`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf216b10",
   "metadata": {},
   "source": [
    "#### Similarity-based QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7b4cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The party was held under the moon and was planned by both toads and frogs.\n"
     ]
    }
   ],
   "source": [
    "# manual creation of the \"retriever\" with the 'similarity' search type\n",
    "retrieverSim = myCassandraVStore.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={\n",
    "        'k': 2,\n",
    "        # ...\n",
    "    },\n",
    ")\n",
    "# Create a \"RetrievalQA\" chain\n",
    "chainSim = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retrieverSim,\n",
    ")\n",
    "# Run it and print results\n",
    "responseSim = chainSim.run(QUESTION)\n",
    "print(responseSim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9886b9",
   "metadata": {},
   "source": [
    "#### MMR-based QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91f0365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The party was held under the moon and was attended by both frogs and toads. Everyone was required to wear a purple hat.\n"
     ]
    }
   ],
   "source": [
    "# manual creation of the \"retriever\" with the 'MMR' search type\n",
    "retrieverMMR = myCassandraVStore.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={\n",
    "        'k': 2,\n",
    "        # ...\n",
    "    },\n",
    ")\n",
    "# Create a \"RetrievalQA\" chain\n",
    "chainMMR = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retrieverMMR\n",
    ")\n",
    "# Run it and print results\n",
    "responseMMR = chainMMR.run(QUESTION)\n",
    "print(responseMMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21da6c",
   "metadata": {},
   "source": [
    "#### Answers with sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44e660",
   "metadata": {},
   "source": [
    "You can run the variant of these chains that also returns the source for the documents used in preparing the answer, which makes it even more obvious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8df6698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity-based chain:\n",
      "  ANSWER : The toads and frogs were planning a moonlit celebration.\n",
      "  SOURCES: Barney's story at the pub\n"
     ]
    }
   ],
   "source": [
    "chainSimSrc = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retrieverSim,\n",
    ")\n",
    "#\n",
    "responseSimSrc = chainSimSrc({chainSimSrc.question_key: QUESTION})\n",
    "print('Similarity-based chain:')\n",
    "print(f'  ANSWER : {responseSimSrc[\"answer\"].strip()}')\n",
    "print(f'  SOURCES: {responseSimSrc[\"sources\"].strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b6bbf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMR-based chain:\n",
      "  ANSWER : The party that night was thrown by frogs and toads, and everyone was required to wear a purple hat.\n",
      "  SOURCES: Barney's story at the pub, The chronicles at the village library\n"
     ]
    }
   ],
   "source": [
    "chainMMRSrc = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retrieverMMR,\n",
    ")\n",
    "#\n",
    "responseMMRSrc = chainMMRSrc({chainMMRSrc.question_key: QUESTION})\n",
    "print('MMR-based chain:')\n",
    "print(f'  ANSWER : {responseMMRSrc[\"answer\"].strip()}')\n",
    "print(f'  SOURCES: {responseMMRSrc[\"sources\"].strip()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What now?\n",
    "\n",
    "This demo is hosted [here](https://cassio.org/frameworks/langchain/qa-maximal-marginal-relevance/) at cassio.org.\n",
    "\n",
    "Discover the other ways you can integrate \n",
    "Cassandra/Astra DB with your ML/GenAI needs,\n",
    "right **within [your favorite framework](https://cassio.org/frameworks/langchain/about/)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
