{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorStore/Q&A, quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab-specific setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a Database, a Secure Connect Bundle and a Database Token. See [DB Setup](https://cassio.org/db_setup/) on cassio.org for details, **paying attention to the notes for Colab users**.\n",
    "Get ready to upload the Bundle and supply the Token string.\n",
    "\n",
    "Likewise, ensure you have the necessary secret for the LLM provider of your choice: you'll be asked to input it shortly. See [API Setup](https://cassio.org/api_setup/) on cassio.org for details, **paying attention to the notes for Colab users**.\n",
    "\n",
    "_Note: this colab is autogenerated from a regular Jupyter notebook hosted by cassio.org. Run all cells in this section to complete the setup before moving on to the demo content proper._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required dependencies\n",
    "! pip install \\\n",
    "    \"git+https://github.com/hemidactylus/langchain@cassio#egg=langchain\" \\\n",
    "    \"git+https://github.com/datastax/python-driver.git@cep-vsearch#egg=cassandra-driver\" \\\n",
    "    \"cassio>=0.0.2\" \\\n",
    "    \"google-cloud-aiplatform>=1.25.0\" \\\n",
    "    \"jupyter>=1.0.0\" \\\n",
    "    \"openai==0.27.7\" \\\n",
    "    \"python-dotenv==1.0.0\" \\\n",
    "    \"tensorflow-cpu==2.12.0\" \\\n",
    "    \"tiktoken==0.4.0\" \\\n",
    "    \"transformers>=4.29.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your database keyspace name:\n",
    "ASTRA_DB_KEYSPACE = input('Your Astra DB Keyspace name: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your Astra DB token string, the one starting with \"AstraCS:...\"\n",
    "ASTRA_DB_APPLICATION_TOKEN = input('Your Astra DB Token: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your Secure Connect Bundle zipfile:\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "print('Please upload your Secure Connect Bundle')\n",
    "uploaded = files.upload()\n",
    "if uploaded:\n",
    "    astraBundleFileTitle = list(uploaded.keys())[0]\n",
    "    ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your secret(s) for LLM access (key names must match `providerValidator` in `llm_choice`)\n",
    "llmProvider = 'OpenAI'  # 'VertexAI'\n",
    "if llmProvider == 'OpenAI':\n",
    "    apiSecret = input(f'Your secret for LLM provider \"{llmProvider}\": ')\n",
    "    os.environ['OPENAI_API_KEY'] = apiSecret\n",
    "elif llmProvider == 'VertexAI':\n",
    "    # we need a json file\n",
    "    print(f'Please upload your Service Account JSON for the LLM provider \"{llmProvider}\":')\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    if uploaded:\n",
    "        vertexAIJsonFileTitle = list(uploaded.keys())[0]\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(os.getcwd(), vertexAIJsonFileTitle)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'No file uploaded. Please re-run the cell.'\n",
    "        )\n",
    "else:\n",
    "    raise ValueError('Unknown/unsupported LLM Provider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab-specific override of helper functions\n",
    "from cassandra.cluster import (\n",
    "    Cluster,\n",
    ")\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "ASTRA_DB_CLIENT_ID = 'token'\n",
    "\n",
    "\n",
    "def getCQLSession(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        cluster = Cluster(\n",
    "            cloud={\n",
    "                \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
    "            },\n",
    "            auth_provider=PlainTextAuthProvider(\n",
    "                ASTRA_DB_CLIENT_ID,\n",
    "                ASTRA_DB_APPLICATION_TOKEN,\n",
    "            ),\n",
    "        )\n",
    "        astraSession = cluster.connect()\n",
    "        return astraSession\n",
    "    else:\n",
    "        raise ValueError('Unknown CQL Session mode')\n",
    "\n",
    "def getCQLKeyspace(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        return ASTRA_DB_KEYSPACE\n",
    "    else:\n",
    "        raise ValueError('Unknown CQL Session mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab preamble completed\n",
    "\n",
    "The following cells constitute the demo notebook proper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6715bc2b",
   "metadata": {},
   "source": [
    "# VectorStore/Q&A, quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d9b70",
   "metadata": {},
   "source": [
    "_**NOTE:** this uses Cassandra's experimental \"Vector Similarity Search\" capability.\n",
    "Make sure you are connecting to a vector-enabled database for this demo._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042f832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4388ac1d",
   "metadata": {},
   "source": [
    "The following line imports the Cassandra flavor of a LangChain vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65c46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.cassandra import Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578a87b",
   "metadata": {},
   "source": [
    "As usual, a database connection is needed to access Cassandra. The following assumes\n",
    "that a _vector-search-capable Cassandra cluster_ is running locally. Adjust as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11013224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the DB connection\n",
    "cqlMode = 'astra_db'\n",
    "session = getCQLSession(mode=cqlMode)\n",
    "keyspace = getCQLKeyspace(mode=cqlMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2a156",
   "metadata": {},
   "source": [
    "Both an LLM and an embedding function are required.\n",
    "\n",
    "Below is the logic to instantiate the LLM and embeddings of choice. We choose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "124e3de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM+embeddings from OpenAI\n"
     ]
    }
   ],
   "source": [
    "# creation of the LLM resources\n",
    "\n",
    "\n",
    "if llmProvider == 'VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    from langchain.embeddings import VertexAIEmbeddings\n",
    "    llm = VertexAI()\n",
    "    myEmbedding = VertexAIEmbeddings()\n",
    "    print('LLM+embeddings from VertexAI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = OpenAI(temperature=0)\n",
    "    myEmbedding = OpenAIEmbeddings()\n",
    "    print('LLM+embeddings from OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f29cf",
   "metadata": {},
   "source": [
    "## A minimal example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf74a31",
   "metadata": {},
   "source": [
    "The following is a minimal usage of the Cassandra vector store. The store is created and filled at once, and is then queried to retrieve relevant parts of the indexed text, which are then stuffed into a prompt finally used to answer a question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237386b",
   "metadata": {},
   "source": [
    "**Note**: for the time being you have to explicitly _turn on this experimental flag_ on the `cassio` side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e7a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassio\n",
    "cassio.globals.enableExperimentalVectorSearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29fc57",
   "metadata": {},
   "source": [
    "The following creates an \"index creator\", which knows about the type of vector store, the embedding to use and how to preprocess the input text:\n",
    "\n",
    "_(Note: stores built with different embedding functions will need different tables. This is why we append the `llmProvider` name to the table name in the next cell.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2cfe71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Cassandra,\n",
    "    embedding=myEmbedding,\n",
    "    text_splitter=CharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=0,\n",
    "    ),\n",
    "    vectorstore_kwargs={\n",
    "        'session': session,\n",
    "        'keyspace': keyspace,\n",
    "        'table_name': 'vs_test1_' + llmProvider,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ddfb6",
   "metadata": {},
   "source": [
    "Loading a local text (a short story by E. A. Poe will do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de8d65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('texts/amontillado.txt', encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53431eca",
   "metadata": {},
   "source": [
    "This takes a few seconds to run, as it must calculate embedding vectors for a number of chunks of the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4929f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is a temporary replacement for next cell, pending resolution of this error from VertexAI:\n",
    "#    \"InvalidArgument: 400 5 instance(s) is allowed per prediction. Actual: <N>\"\n",
    "# (i.e. one cannot request embedding of more than 5 strings in a single call)\n",
    "if llmProvider == 'VertexAI':\n",
    "    from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "    docs = loader.load()\n",
    "    subdocs = index_creator.text_splitter.split_documents(docs)\n",
    "    #\n",
    "    print(f'subdocument {0} ...', end=' ')\n",
    "    vs = index_creator.vectorstore_cls.from_documents(\n",
    "        subdocs[:1],\n",
    "        index_creator.embedding,\n",
    "        **index_creator.vectorstore_kwargs,\n",
    "    )\n",
    "    print('done.')\n",
    "    for sdi, sd in enumerate(subdocs[1:]):\n",
    "        print(f'subdocument {sdi+1} ...', end=' ')\n",
    "        vs.add_texts(texts=[sd.page_content], metadata=[sd.metadata])\n",
    "        print('done.')\n",
    "    #\n",
    "    index = VectorStoreIndexWrapper(vectorstore=vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c9970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if llmProvider != 'VertexAI':\n",
    "    index = index_creator.from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f35aaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Luchesi is a friend of Fortunato's who has a critical turn and is known for his taste in wine.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who is Luchesi?\"\n",
    "index.query(query, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a4697",
   "metadata": {},
   "source": [
    "## Spawning a \"retriever\" from the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bae5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.vectorstore.as_retriever(search_kwargs={\n",
    "    'k': 2,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4e816ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\"A huge human foot d\\'or, in a field azure; the foot crushes a serpent\\nrampant whose fangs are imbedded in the heel.\"\\n\\n\"And the motto?\"\\n\\n\"_Nemo me impune lacessit_.\"\\n\\n\"Good!\" he said.', metadata={'source': 'texts/amontillado.txt'}),\n",
       " Document(page_content='He raised it to his lips with a leer.  He paused and nodded to me\\nfamiliarly, while his bells jingled.\\n\\n\"I drink,\" he said, \"to the buried that repose around us.\"\\n\\n\"And I to your long life.\"\\n\\nHe again took my arm, and we proceeded.\\n\\n\"These vaults,\" he said, \"are extensive.\"\\n\\n\"The Montresors,\" I replied, \"were a great and numerous family.\"\\n\\n\"I forget your arms.\"', metadata={'source': 'texts/amontillado.txt'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\n",
    "    \"Check the motto of the Montresors\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
