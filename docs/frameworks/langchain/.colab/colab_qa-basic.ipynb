{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Similarity Search QA Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab-specific setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a Database and get ready to upload the Secure Connect Bundle and supply the Token string\n",
    "(see [Pre-requisites](https://cassio.org/start_here/#vector-database) on cassio.org for details. Remember you need a **custom Token** with role [Database Administrator](https://awesome-astra.github.io/docs/pages/astra/create-token/)).\n",
    "\n",
    "Likewise, ensure you have the necessary secret for the LLM provider of your choice: you'll be asked to input it shortly\n",
    "(see [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org for details).\n",
    "\n",
    "_Note: this notebook is part of the CassIO documentation. Visit [this page on cassIO.org](https://cassio.org/frameworks/langchain/qa-basic/)._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required dependencies\n",
    "! pip install \\\n",
    "    \"git+https://github.com/hemidactylus/langchain@cassio-full-preview#egg=langchain\" \\\n",
    "    \"cassandra-driver>=3.28.0\" \\\n",
    "    \"cassio==0.0.6\" \\\n",
    "    \"google-cloud-aiplatform>=1.25.0\" \\\n",
    "    \"jupyter>=1.0.0\" \\\n",
    "    \"openai==0.27.7\" \\\n",
    "    \"python-dotenv==1.0.0\" \\\n",
    "    \"tensorflow-cpu==2.12.0\" \\\n",
    "    \"tiktoken==0.4.0\" \\\n",
    "    \"transformers>=4.29.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f44ff",
   "metadata": {},
   "source": [
    "You will likely be asked to \"Restart the Runtime\" at this time, as some dependencies\n",
    "have been upgraded. **Please do restart the runtime now** for a smoother execution from this point onward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your database keyspace name:\n",
    "ASTRA_DB_KEYSPACE = input('Your Astra DB Keyspace name (e.g. cassio_tutorials): ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your Astra DB token string, the one starting with \"AstraCS:...\"\n",
    "from getpass import getpass\n",
    "ASTRA_DB_TOKEN_BASED_PASSWORD = getpass('Your Astra DB Token (\"AstraCS:...\"): ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNQ6T_Gjk0Oz"
   },
   "source": [
    "### Astra DB Secure Connect Bundle\n",
    "\n",
    "Please upload the Secure Connect Bundle zipfile to connect to your Astra DB instance.\n",
    "\n",
    "The Secure Connect Bundle is needed to establish a secure connection to the database.\n",
    "Click [here](https://awesome-astra.github.io/docs/pages/astra/download-scb/#c-procedure) for instructions on how to download it from Astra DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your Secure Connect Bundle zipfile:\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "print('Please upload your Secure Connect Bundle')\n",
    "uploaded = files.upload()\n",
    "if uploaded:\n",
    "    astraBundleFileTitle = list(uploaded.keys())[0]\n",
    "    ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab-specific override of helper functions\n",
    "from cassandra.cluster import (\n",
    "    Cluster,\n",
    ")\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "# The \"username\" is the literal string 'token' for this connection mode:\n",
    "ASTRA_DB_TOKEN_BASED_USERNAME = 'token'\n",
    "\n",
    "\n",
    "def getCQLSession(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        cluster = Cluster(\n",
    "            cloud={\n",
    "                \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
    "            },\n",
    "            auth_provider=PlainTextAuthProvider(\n",
    "                ASTRA_DB_TOKEN_BASED_USERNAME,\n",
    "                ASTRA_DB_TOKEN_BASED_PASSWORD,\n",
    "            ),\n",
    "        )\n",
    "        astraSession = cluster.connect()\n",
    "        return astraSession\n",
    "    else:\n",
    "        raise ValueError('Unsupported CQL Session mode')\n",
    "\n",
    "def getCQLKeyspace(mode='astra_db'):\n",
    "    if mode == 'astra_db':\n",
    "        return ASTRA_DB_KEYSPACE\n",
    "    else:\n",
    "        raise ValueError('Unsupported CQL Session mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXCQ6T_Gjk0Oz"
   },
   "source": [
    "### LLM Provider\n",
    "\n",
    "In the cell below you can choose between **GCP Vertex AI** or **OpenAI** for your LLM services.\n",
    "(See [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org for more details).\n",
    "\n",
    "Make sure you set the `llmProvider` variable and supply the corresponding access secrets in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your secret(s) for LLM access:\n",
    "llmProvider = 'OpenAI'  # 'GCP_VertexAI', 'Azure_OpenAI'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "if llmProvider == 'OpenAI':\n",
    "    apiSecret = getpass(f'Your secret for LLM provider \"{llmProvider}\": ')\n",
    "    os.environ['OPENAI_API_KEY'] = apiSecret\n",
    "elif llmProvider == 'GCP_VertexAI':\n",
    "    # we need a json file\n",
    "    print(f'Please upload your Service Account JSON for the LLM provider \"{llmProvider}\":')\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    if uploaded:\n",
    "        vertexAIJsonFileTitle = list(uploaded.keys())[0]\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(os.getcwd(), vertexAIJsonFileTitle)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'No file uploaded. Please re-run the cell.'\n",
    "        )\n",
    "elif llmProvider == 'Azure_OpenAI':\n",
    "    # a few parameters must be input\n",
    "    apiSecret = input(f'Your API Key for LLM provider \"{llmProvider}\": ')\n",
    "    os.environ['AZURE_OPENAI_API_KEY'] = apiSecret\n",
    "    apiBase = input('The \"Base URL\" for your models (e.g. \"https://YOUR-RESOURCE-NAME.openai.azure.com\"): ')\n",
    "    os.environ['AZURE_OPENAI_API_BASE'] = apiBase\n",
    "    apiLLMDepl = input('The name of your LLM Deployment: ')\n",
    "    os.environ['AZURE_OPENAI_LLM_DEPLOYMENT'] = apiLLMDepl\n",
    "    apiLLMModel = input('The name of your LLM Model (e.g. \"gpt-4\"): ')\n",
    "    os.environ['AZURE_OPENAI_LLM_MODEL'] = apiLLMModel\n",
    "    apiEmbDepl = input('The name for your Embeddings Deployment: ')\n",
    "    os.environ['AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT'] = apiEmbDepl\n",
    "    apiEmbModel = input('The name of your Embedding Model (e.g. \"text-embedding-ada-002\"): ')\n",
    "    os.environ['AZURE_OPENAI_EMBEDDINGS_MODEL'] = apiEmbModel\n",
    "\n",
    "    # The following is probably not going to change for some time...\n",
    "    os.environ['AZURE_OPENAI_API_VERSION'] = '2023-03-15-preview'\n",
    "else:\n",
    "    raise ValueError('Unknown/unsupported LLM Provider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the text of a short story that will be indexed in the vector store\n",
    "! curl https://raw.githubusercontent.com/CassioML/cassio-website/main/docs/frameworks/langchain/texts/amontillado.txt --output amontillado.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab preamble completed\n",
    "\n",
    "The following cells constitute the demo notebook proper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6715bc2b",
   "metadata": {},
   "source": [
    "# Vector Similarity Search QA Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d9b70",
   "metadata": {},
   "source": [
    "_**NOTE:** this uses Cassandra's \"Vector Similarity Search\" capability.\n",
    "Make sure you are connecting to a vector-enabled database for this demo._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042f832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4388ac1d",
   "metadata": {},
   "source": [
    "The following line imports the Cassandra flavor of a LangChain vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65c46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.cassandra import Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578a87b",
   "metadata": {},
   "source": [
    "A database connection is needed to access Cassandra. The following assumes\n",
    "that a _vector-search-capable Astra DB instance_ is available. Adjust as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11013224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the DB connection\n",
    "cqlMode = 'astra_db'\n",
    "session = getCQLSession(mode=cqlMode)\n",
    "keyspace = getCQLKeyspace(mode=cqlMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2a156",
   "metadata": {},
   "source": [
    "Both an LLM and an embedding function are required.\n",
    "\n",
    "Below is the logic to instantiate the LLM and embeddings of choice. We chose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "124e3de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM+embeddings from Vertex AI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# creation of the LLM resources\n",
    "\n",
    "\n",
    "if llmProvider == 'GCP_VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    from langchain.embeddings import VertexAIEmbeddings\n",
    "    llm = VertexAI()\n",
    "    myEmbedding = VertexAIEmbeddings()\n",
    "    print('LLM+embeddings from Vertex AI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    os.environ['OPENAI_API_TYPE'] = 'open_ai'\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = OpenAI(temperature=0)\n",
    "    myEmbedding = OpenAIEmbeddings()\n",
    "    print('LLM+embeddings from OpenAI')\n",
    "elif llmProvider == 'Azure_OpenAI':\n",
    "    os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "    os.environ['OPENAI_API_VERSION'] = os.environ['AZURE_OPENAI_API_VERSION']\n",
    "    os.environ['OPENAI_API_BASE'] = os.environ['AZURE_OPENAI_API_BASE']\n",
    "    os.environ['OPENAI_API_KEY'] = os.environ['AZURE_OPENAI_API_KEY']\n",
    "    from langchain.llms import AzureOpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = AzureOpenAI(temperature=0, model_name=os.environ['AZURE_OPENAI_LLM_MODEL'],\n",
    "                      engine=os.environ['AZURE_OPENAI_LLM_DEPLOYMENT'])\n",
    "    myEmbedding = OpenAIEmbeddings(model=os.environ['AZURE_OPENAI_EMBEDDINGS_MODEL'],\n",
    "                                   deployment=os.environ['AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT'])\n",
    "    print('LLM+embeddings from Azure OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f29cf",
   "metadata": {},
   "source": [
    "## A minimal example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf74a31",
   "metadata": {},
   "source": [
    "The following is a minimal usage of the Cassandra vector store. The store is created and filled at once, and is then queried to retrieve relevant parts of the indexed text, which are then stuffed into a prompt finally used to answer a question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29fc57",
   "metadata": {},
   "source": [
    "The following creates an \"index creator\", which knows about the type of vector store, the embedding to use and how to preprocess the input text:\n",
    "\n",
    "_(Note: stores built with different embedding functions will need different tables. This is why we append the `llmProvider` name to the table name in the next cell.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2cfe71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'vs_test1_' + llmProvider\n",
    "\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Cassandra,\n",
    "    embedding=myEmbedding,\n",
    "    text_splitter=CharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=0,\n",
    "    ),\n",
    "    vectorstore_kwargs={\n",
    "        'session': session,\n",
    "        'keyspace': keyspace,\n",
    "        'table_name': table_name,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ddfb6",
   "metadata": {},
   "source": [
    "Loading a local text (a short story by E. A. Poe will do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de8d65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('amontillado.txt', encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53431eca",
   "metadata": {},
   "source": [
    "This takes a few seconds to run, as it must calculate embedding vectors for a number of chunks of the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4929f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Certain LLM providers need workaround to evaluate batch embeddings\n",
    "#       (as done in next cell).\n",
    "#       As of 2023-06-29, Azure OpenAI would  error with:\n",
    "#           \"InvalidRequestError: Too many inputs. The max number of inputs is 1\"\n",
    "if llmProvider == 'Azure_OpenAI':\n",
    "    from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "    docs = loader.load()\n",
    "    subdocs = index_creator.text_splitter.split_documents(docs)\n",
    "    #\n",
    "    print(f'subdocument {0} ...', end=' ')\n",
    "    vs = index_creator.vectorstore_cls.from_documents(\n",
    "        subdocs[:1],\n",
    "        index_creator.embedding,\n",
    "        **index_creator.vectorstore_kwargs,\n",
    "    )\n",
    "    print('done.')\n",
    "    for sdi, sd in enumerate(subdocs[1:]):\n",
    "        print(f'subdocument {sdi+1} ...', end=' ')\n",
    "        vs.add_texts(texts=[sd.page_content], metadata=[sd.metadata])\n",
    "        print('done.')\n",
    "    #\n",
    "    index = VectorStoreIndexWrapper(vectorstore=vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64c9970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if llmProvider != 'Azure_OpenAI':\n",
    "    index = index_creator.from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c02057",
   "metadata": {},
   "source": [
    "### Check what's on DB\n",
    "\n",
    "By way of demonstration, if you were to directly read the rows stored in your database table, this is what you would now find there (not that you'll ever _have to_, for LangChain and CassIO provide an abstraction on top of that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de08db04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Row 0:\n",
      "    document_id:      21fbd9985564f7f12ac51f4c20232d75\n",
      "    embedding_vector: [-0.011485965922474861, -0.01858605071902275, 0.0115145826712250 ...\n",
      "    document:         \"Pass your hand,\" I said, \"over the wall; you cannot help feelin ...\n",
      "    metadata_blob:    {\"source\": \"texts/amontillado.txt\"}\n",
      "\n",
      "Row 1:\n",
      "    document_id:      f5020721820969b3fbf6b12691818508\n",
      "    embedding_vector: [0.011451096273958683, -0.006945343688130379, -0.007215586956590 ...\n",
      "    document:         No answer still.  I thrust a torch through the remaining apertur ...\n",
      "    metadata_blob:    {\"source\": \"texts/amontillado.txt\"}\n",
      "\n",
      "Row 2:\n",
      "    document_id:      d2ff9ab96b181d455481c67f84558091\n",
      "    embedding_vector: [-0.0056611113250255585, -0.0022278032265603542, 0.0493778288364 ...\n",
      "    document:         I said to him--\"My dear Fortunato, you are luckily met.  How rem ...\n",
      "    metadata_blob:    {\"source\": \"texts/amontillado.txt\"}\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "cqlSelect = f'SELECT * FROM {keyspace}.{table_name} LIMIT 3;'  # (Not a production-optimized query ...)\n",
    "rows = session.execute(cqlSelect)\n",
    "for row_i, row in enumerate(rows):\n",
    "    print(f'\\nRow {row_i}:')\n",
    "    print(f'    document_id:      {row.document_id}')\n",
    "    print(f'    embedding_vector: {str(row.embedding_vector)[:64]} ...')\n",
    "    print(f'    document:         {row.document[:64]} ...')\n",
    "    print(f'    metadata_blob:    {row.metadata_blob}')\n",
    "\n",
    "print('\\n...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d9f48b",
   "metadata": {},
   "source": [
    "### Ask a question, get an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f35aaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Luchesi is a wine critic.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who is Luchesi?\"\n",
    "index.query(query, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a4697",
   "metadata": {},
   "source": [
    "## Spawning a \"retriever\" from the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6b732",
   "metadata": {},
   "source": [
    "You just saw how easily you can plug a Cassandra-backed Vector Index into a full question-answering LangChain pipeline.\n",
    "\n",
    "But you can as easily work at a slightly lower level: the following code spawns a `VectorStoreRetriever` from the index for manual [retrieval](https://python.langchain.com/en/latest/modules/indexes/retrievers.html) of documents related to a given query text. The results are instances of LangChain's `Document` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bae5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.vectorstore.as_retriever(search_kwargs={\n",
    "    'k': 2,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4e816ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='He raised it to his lips with a leer.  He paused and nodded to me\\nfamiliarly, while his bells jingled.\\n\\n\"I drink,\" he said, \"to the buried that repose around us.\"\\n\\n\"And I to your long life.\"\\n\\nHe again took my arm, and we proceeded.\\n\\n\"These vaults,\" he said, \"are extensive.\"\\n\\n\"The Montresors,\" I replied, \"were a great and numerous family.\"\\n\\n\"I forget your arms.\"', metadata={'source': 'texts/amontillado.txt'}),\n",
       " Document(page_content='\"A huge human foot d\\'or, in a field azure; the foot crushes a serpent\\nrampant whose fangs are imbedded in the heel.\"\\n\\n\"And the motto?\"\\n\\n\"_Nemo me impune lacessit_.\"\\n\\n\"Good!\" he said.', metadata={'source': 'texts/amontillado.txt'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\n",
    "    \"Check the motto of the Montresors\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What now?\n",
    "\n",
    "This demo is hosted [here](https://cassio.org/frameworks/langchain/qa-basic/) at cassio.org.\n",
    "\n",
    "Discover the other ways you can integrate \n",
    "Cassandra/Astra DB with your ML/GenAI needs,\n",
    "right **within [your favorite framework](https://cassio.org/frameworks/langchain/about/)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
