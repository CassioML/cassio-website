{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84094469",
   "metadata": {},
   "source": [
    "# Caching LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddaf62",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use Cassandra for a basic prompt/response cache.\n",
    "\n",
    "Such a cache prevents running an LLM invocation more than once for the very same prompt, thus saving on latency and token usage. The cache retrieval logic is based on an exact match, as will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec81edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import CassandraCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578a87b",
   "metadata": {},
   "source": [
    "A database connection is needed. _(If on a Colab, the only supported option is the cloud service Astra DB.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9515d4af-9fc2-4c29-b9f2-5c90cd48cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure loading of database credentials into environment variables:\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../../../.env\")\n",
    "\n",
    "import cassio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ee44a4-8d6a-4b41-8a8f-7c93eed35034",
   "metadata": {},
   "source": [
    "Select your choice of database by editing this cell, if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe78b92-7194-4ee9-ba7f-cd308409212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_mode = \"cassandra\"  # \"cassandra\" / \"astra_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5597b3-a028-4d8f-ad6d-8eaf0c941dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if database_mode == \"astra_db\":\n",
    "    cassio.init(\n",
    "        database_id=os.environ[\"ASTRA_DB_ID\"],\n",
    "        token=os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"],\n",
    "        keyspace=os.environ.get(\"ASTRA_DB_KEYSPACE\"),  # this is optional\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "329d969b-6532-4dc4-a315-1a5438a59919",
   "metadata": {},
   "outputs": [],
   "source": [
    "if database_mode == \"cassandra\":\n",
    "    from cqlsession import getCassandraCQLSession, getCassandraCQLKeyspace\n",
    "    cassio.init(\n",
    "        session=getCassandraCQLSession(),\n",
    "        keyspace=getCassandraCQLKeyspace(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65bff51",
   "metadata": {},
   "source": [
    "Create a `CassandraCache` and configure it globally for LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "445307fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.llm_cache = CassandraCache(\n",
    "    session=None,\n",
    "    keyspace=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b7e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.llm_cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b619bb",
   "metadata": {},
   "source": [
    "Below is the logic to instantiate the LLM of choice. We chose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a533119f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM from OpenAI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llm_choice import suggestLLMProvider\n",
    "\n",
    "llmProvider = suggestLLMProvider()\n",
    "# (Alternatively set llmProvider to 'GCP_VertexAI', 'OpenAI', 'Azure_OpenAI' ... manually if you have credentials)\n",
    "\n",
    "if llmProvider == 'GCP_VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    llm = VertexAI()\n",
    "    print('LLM from Vertex AI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    os.environ['OPENAI_API_TYPE'] = 'open_ai'\n",
    "    from langchain.llms import OpenAI\n",
    "    llm = OpenAI()\n",
    "    print('LLM from OpenAI')\n",
    "elif llmProvider == 'Azure_OpenAI':\n",
    "    os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "    os.environ['OPENAI_API_VERSION'] = os.environ['AZURE_OPENAI_API_VERSION']\n",
    "    os.environ['OPENAI_API_BASE'] = os.environ['AZURE_OPENAI_API_BASE']\n",
    "    os.environ['OPENAI_API_KEY'] = os.environ['AZURE_OPENAI_API_KEY']\n",
    "    from langchain.llms import AzureOpenAI\n",
    "    llm = AzureOpenAI(temperature=0, model_name=os.environ['AZURE_OPENAI_LLM_MODEL'],\n",
    "                      engine=os.environ['AZURE_OPENAI_LLM_DEPLOYMENT'])\n",
    "    print('LLM from Azure OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f17cce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 ms, sys: 1.74 ms, total: 19.5 ms\n",
      "Wall time: 459 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders typically have eight eyes.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_1 = \"How many eyes do spiders have?\"\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47bc4bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.73 ms, sys: 634 µs, total: 2.36 ms\n",
      "Wall time: 2.46 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders typically have eight eyes.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# This time we expect a much shorter answer time\n",
    "llm(SPIDER_QUESTION_FORM_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88d657dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.27 ms, sys: 3 ms, total: 8.26 ms\n",
      "Wall time: 644 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders typically have eight eyes, arranged in two rows of four.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SPIDER_QUESTION_FORM_2 = \"How many eyes do spiders generally have?\"\n",
    "# This will again take 1-2 seconds, being a different string\n",
    "llm(SPIDER_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc048dcb",
   "metadata": {},
   "source": [
    "### Caching and Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4bc6c4",
   "metadata": {},
   "source": [
    "The `CassandraCache` supports caching within chat-oriented LangChain abstractions such as `ChatOpenAI` as well:\n",
    "\n",
    "_(warning: the following is demonstrated **with OpenAI only** for the time being)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ac6e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e77d76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, there are no spiders with wings. Spiders belong to the class Arachnida, which includes creatures with eight legs and no wings. They rely on their silk-producing abilities to create webs and catch prey, rather than flying.\n",
      "CPU times: user 10.5 ms, sys: 2.37 ms, total: 12.9 ms\n",
      "Wall time: 3.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(chat_llm.predict(\"Are there spiders with wings?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf2950e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, there are no spiders with wings. Spiders belong to the class Arachnida, which includes creatures with eight legs and no wings. They rely on their silk-producing abilities to create webs and catch prey, rather than flying.\n",
      "CPU times: user 4.38 ms, sys: 139 µs, total: 4.52 ms\n",
      "Wall time: 4.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Expect a much faster response:\n",
    "print(chat_llm.predict(\"Are there spiders with wings?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a98b40c",
   "metadata": {},
   "source": [
    "(Actually, every object which inherits from the LangChain `Generation` class can be seamlessly store and retrieved in this cache.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048a336",
   "metadata": {},
   "source": [
    "### Stale entry control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca8cd5",
   "metadata": {},
   "source": [
    "#### Time-To-Live (TTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de2adf",
   "metadata": {},
   "source": [
    "You can configure a time-to-live property of the cache, with the effect of automatic eviction of cached entries after a certain time.\n",
    "\n",
    "Setting `langchain.llm_cache` to the following will have the effect that entries vanish in an hour (also supplying a custom table name is demonstrated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faec79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheWithTTL = CassandraCache(\n",
    "    session=None,\n",
    "    keyspace=None,\n",
    "    table_name=\"langchain_llm_cache\",\n",
    "    ttl_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8ec77",
   "metadata": {},
   "source": [
    "#### Manual cache eviction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b414d43",
   "metadata": {},
   "source": [
    "Alternatively, you can invalidate cached entries one at a time - for that, you'll need to provide the very LLM this entry is associated to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56df34d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.04 ms, sys: 0 ns, total: 3.04 ms\n",
      "Wall time: 3.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders typically have eight eyes, arranged in two rows of four.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(SPIDER_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d42dcb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.llm_cache.delete_through_llm(SPIDER_QUESTION_FORM_2, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ed25580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 ms, sys: 824 µs, total: 11.1 ms\n",
      "Wall time: 1.04 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSpiders typically have eight eyes, although some species may have as few as two or as many as twelve.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(SPIDER_QUESTION_FORM_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3193c67",
   "metadata": {},
   "source": [
    "#### Whole-cache deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d74e6",
   "metadata": {},
   "source": [
    "As you might have seen at the beginning of this notebook, you can also clear the cache entirely: **all** stored entries, for all models, will be evicted at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff899ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.llm_cache.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
