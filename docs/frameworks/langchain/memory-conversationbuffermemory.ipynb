{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4174d541",
   "metadata": {},
   "source": [
    "# ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a0b5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import CassandraChatMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd0c9d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/cassandra/io/asyncorereactor.py:34: DeprecationWarning: The asyncore module is deprecated and will be removed in Python 3.12. The recommended replacement is asyncio\n",
      "  import asyncore\n",
      "/home/stefano/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/cassandra/datastax/cloud/__init__.py:173: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n",
      "  ssl_context = SSLContext(PROTOCOL_TLS)\n",
      "/home/stefano/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/cassandra/io/asyncorereactor.py:347: DeprecationWarning: ssl.match_hostname() is deprecated\n",
      "  self._connect_socket()\n"
     ]
    }
   ],
   "source": [
    "from cqlsession import getCQLSession, getCQLKeyspace\n",
    "astraSession = getCQLSession()\n",
    "astraKeyspace = getCQLKeyspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367ba093",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = CassandraChatMessageHistory(\n",
    "    session_id='conversation-0123',\n",
    "    session=astraSession,\n",
    "    keyspace='langchain',\n",
    "    ttl_seconds = 3600,\n",
    ")\n",
    "message_history.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc9ab2",
   "metadata": {},
   "source": [
    "## Use in a ConversationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8a0a2",
   "metadata": {},
   "source": [
    "### Create a Memory\n",
    "\n",
    "The Cassandra message history, and the corresponding template variable name for filling prompts, are specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55b0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassBuffMemory = ConversationBufferMemory(\n",
    "    chat_memory=message_history,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8a7a9",
   "metadata": {},
   "source": [
    "### Language model\n",
    "\n",
    "We need an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cad98fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b62ac",
   "metadata": {},
   "source": [
    "### Create a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "831efb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=cassBuffMemory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "334080a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello, how can I roast an apple?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! To roast an apple, you'll need to preheat your oven to 375 degrees Fahrenheit. Cut the apple into slices or wedges and spread them out on a baking sheet. Sprinkle with cinnamon and brown sugar. Bake for 30 minutes, or until the apple slices are tender. Enjoy your roasted apples!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hello, how can I roast an apple?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b3fef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, how can I roast an apple?\n",
      "AI:  Hi there! To roast an apple, you'll need to preheat your oven to 375 degrees Fahrenheit. Cut the apple into slices or wedges and spread them out on a baking sheet. Sprinkle with cinnamon and brown sugar. Bake for 30 minutes, or until the apple slices are tender. Enjoy your roasted apples!\n",
      "Human: Can I do it on a bonfire?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, I don't know the answer to that question. Roasting an apple requires an oven.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Can I do it on a bonfire?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f56e1345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, how can I roast an apple?\n",
      "AI:  Hi there! To roast an apple, you'll need to preheat your oven to 375 degrees Fahrenheit. Cut the apple into slices or wedges and spread them out on a baking sheet. Sprinkle with cinnamon and brown sugar. Bake for 30 minutes, or until the apple slices are tender. Enjoy your roasted apples!\n",
      "Human: Can I do it on a bonfire?\n",
      "AI:  I'm sorry, I don't know the answer to that question. Roasting an apple requires an oven.\n",
      "Human: What about a microwave, would the apple taste good?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' No, microwaving an apple is not recommended. The result would be a soggy apple with no texture. Roasting is the best way to get a delicious, caramelized apple.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What about a microwave, would the apple taste good?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a6d877b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello, how can I roast an apple?', additional_kwargs={}, example=False),\n",
       " AIMessage(content=\" Hi there! To roast an apple, you'll need to preheat your oven to 375 degrees Fahrenheit. Cut the apple into slices or wedges and spread them out on a baking sheet. Sprinkle with cinnamon and brown sugar. Bake for 30 minutes, or until the apple slices are tender. Enjoy your roasted apples!\", additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Can I do it on a bonfire?', additional_kwargs={}, example=False),\n",
       " AIMessage(content=\" I'm sorry, I don't know the answer to that question. Roasting an apple requires an oven.\", additional_kwargs={}, example=False),\n",
       " HumanMessage(content='What about a microwave, would the apple taste good?', additional_kwargs={}, example=False),\n",
       " AIMessage(content=' No, microwaving an apple is not recommended. The result would be a soggy apple with no texture. Roasting is the best way to get a delicious, caramelized apple.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc7787",
   "metadata": {},
   "source": [
    "## Manually tinkering with the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3a56bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ea8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a quirky chatbot having a\n",
    "conversation with a human, riddled with puns and silly jokes.\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78ab9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_message_history = CassandraChatMessageHistory(\n",
    "    session_id='conversation-funny-a001',\n",
    "    session=astraSession,\n",
    "    keyspace='langchain',\n",
    ")\n",
    "f_message_history.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cb5e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    chat_memory=f_message_history,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0ba39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt, \n",
    "    verbose=True, \n",
    "    memory=f_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23143e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a quirky chatbot having a\n",
      "conversation with a human, riddled with puns and silly jokes.\n",
      "\n",
      "\n",
      "Human: Tell me about springs\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Springs are full of surprises! They bounce back with lots of energy and are always ready to take on a new challenge. Plus, they provide a great way to relax and have fun!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"Tell me about springs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4e19081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a quirky chatbot having a\n",
      "conversation with a human, riddled with puns and silly jokes.\n",
      "\n",
      "Human: Tell me about springs\n",
      "AI:  Springs are full of surprises! They bounce back with lots of energy and are always ready to take on a new challenge. Plus, they provide a great way to relax and have fun!\n",
      "Human: Er ... I mean the other type actually.\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Oh, you mean the season? Well, spring is a season of renewal and growth. Everything is coming back to life after a cold winter and the world starts to bloom and blossom. It's definitely a time of joy and celebration!\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input='Er ... I mean the other type actually.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
