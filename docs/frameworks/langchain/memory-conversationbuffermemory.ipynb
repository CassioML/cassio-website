{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4174d541",
   "metadata": {},
   "source": [
    "# ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a0b5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import CassandraChatMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd0c9d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/cassandra/datastax/cloud/__init__.py:173: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n",
      "  ssl_context = SSLContext(PROTOCOL_TLS)\n",
      "/home/stefano/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/cassandra/io/asyncorereactor.py:347: DeprecationWarning: ssl.match_hostname() is deprecated\n",
      "  self._connect_socket()\n"
     ]
    }
   ],
   "source": [
    "from cqlsession import getCQLSession, getCQLKeyspace\n",
    "astraSession = getCQLSession()\n",
    "astraKeyspace = getCQLKeyspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367ba093",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = CassandraChatMessageHistory(\n",
    "    session_id='conversation-0123',\n",
    "    session=astraSession,\n",
    "    keyspace='langchain',\n",
    "    ttl_seconds = 3600,\n",
    ")\n",
    "message_history.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc9ab2",
   "metadata": {},
   "source": [
    "## Use in a ConversationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8a0a2",
   "metadata": {},
   "source": [
    "### Create a Memory\n",
    "\n",
    "The Cassandra message history, and the corresponding template variable name for filling prompts, are specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55b0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassBuffMemory = ConversationBufferMemory(\n",
    "    chat_memory=message_history,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8a7a9",
   "metadata": {},
   "source": [
    "### Language model\n",
    "\n",
    "Below is the logic to instantiate the LLM of choice. We choose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cad98fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/google/api_core/operations_v1/abstract_operations_client.py:17: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  from distutils import util\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM from VertexAI\n"
     ]
    }
   ],
   "source": [
    "from llm_choice import suggestLLMProvider\n",
    "\n",
    "llmProvider = suggestLLMProvider()\n",
    "# (Alternatively set llmProvider to 'VertexAI', 'OpenAI' ... manually if you have credentials)\n",
    "\n",
    "if llmProvider == 'VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    llm = VertexAI()\n",
    "    print('LLM from VertexAI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    from langchain.llms import OpenAI\n",
    "    llm = OpenAI()\n",
    "    print('LLM from OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b62ac",
   "metadata": {},
   "source": [
    "### Create a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831efb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=cassBuffMemory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "334080a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello, how can I roast an apple?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Preheat oven to 400 degrees F (200 degrees C). Core and cut apples into 8 wedges. In a large bowl, toss apples with melted butter, brown sugar, cinnamon, and nutmeg. Spread apples in a single layer on a baking sheet. Bake in preheated oven for 20-25 minutes, or until apples are tender and slightly browned.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hello, how can I roast an apple?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b3fef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, how can I roast an apple?\n",
      "AI: Preheat oven to 400 degrees F (200 degrees C). Core and cut apples into 8 wedges. In a large bowl, toss apples with melted butter, brown sugar, cinnamon, and nutmeg. Spread apples in a single layer on a baking sheet. Bake in preheated oven for 20-25 minutes, or until apples are tender and slightly browned.\n",
      "Human: Can I do it on a bonfire?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, you can roast apples on a bonfire. To do this, core and cut the apples into wedges. Place the apples in a foil packet with some butter, brown sugar, cinnamon, and nutmeg. Seal the packet and place it on the coals of the bonfire. Roast the apples for 20-25 minutes, or until they are tender and slightly browned.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Can I do it on a bonfire?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f56e1345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, how can I roast an apple?\n",
      "AI: Preheat oven to 400 degrees F (200 degrees C). Core and cut apples into 8 wedges. In a large bowl, toss apples with melted butter, brown sugar, cinnamon, and nutmeg. Spread apples in a single layer on a baking sheet. Bake in preheated oven for 20-25 minutes, or until apples are tender and slightly browned.\n",
      "Human: Can I do it on a bonfire?\n",
      "AI: Yes, you can roast apples on a bonfire. To do this, core and cut the apples into wedges. Place the apples in a foil packet with some butter, brown sugar, cinnamon, and nutmeg. Seal the packet and place it on the coals of the bonfire. Roast the apples for 20-25 minutes, or until they are tender and slightly browned.\n",
      "Human: What about a microwave, would the apple taste good?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You can also roast apples in the microwave. To do this, core and cut the apples into wedges. Place the apples in a microwave-safe dish with some butter, brown sugar, cinnamon, and nutmeg. Cover the dish with plastic wrap and microwave on high for 5-7 minutes, or until the apples are tender and slightly browned.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What about a microwave, would the apple taste good?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a6d877b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello, how can I roast an apple?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Preheat oven to 400 degrees F (200 degrees C). Core and cut apples into 8 wedges. In a large bowl, toss apples with melted butter, brown sugar, cinnamon, and nutmeg. Spread apples in a single layer on a baking sheet. Bake in preheated oven for 20-25 minutes, or until apples are tender and slightly browned.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Can I do it on a bonfire?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Yes, you can roast apples on a bonfire. To do this, core and cut the apples into wedges. Place the apples in a foil packet with some butter, brown sugar, cinnamon, and nutmeg. Seal the packet and place it on the coals of the bonfire. Roast the apples for 20-25 minutes, or until they are tender and slightly browned.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='What about a microwave, would the apple taste good?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='You can also roast apples in the microwave. To do this, core and cut the apples into wedges. Place the apples in a microwave-safe dish with some butter, brown sugar, cinnamon, and nutmeg. Cover the dish with plastic wrap and microwave on high for 5-7 minutes, or until the apples are tender and slightly browned.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc7787",
   "metadata": {},
   "source": [
    "## Manually tinkering with the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3a56bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9ea8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a quirky chatbot having a\n",
    "conversation with a human, riddled with puns and silly jokes.\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78ab9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_message_history = CassandraChatMessageHistory(\n",
    "    session_id='conversation-funny-a001',\n",
    "    session=astraSession,\n",
    "    keyspace='langchain',\n",
    ")\n",
    "f_message_history.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cb5e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    chat_memory=f_message_history,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0ba39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt, \n",
    "    verbose=True, \n",
    "    memory=f_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23143e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a quirky chatbot having a\n",
      "conversation with a human, riddled with puns and silly jokes.\n",
      "\n",
      "\n",
      "Human: Tell me about springs\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Springs are great for storing energy, just like a trampoline. But be careful not to bounce too high, or you might spring a leak!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"Tell me about springs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4e19081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a quirky chatbot having a\n",
      "conversation with a human, riddled with puns and silly jokes.\n",
      "\n",
      "Human: Tell me about springs\n",
      "AI: Springs are great for storing energy, just like a trampoline. But be careful not to bounce too high, or you might spring a leak!\n",
      "Human: Er ... I mean the other type actually.\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Oh, you mean like the kind that go on your shoes? Those are great for keeping your feet dry, but be careful not to step in a puddle, or you might spring a leak!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input='Er ... I mean the other type actually.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
