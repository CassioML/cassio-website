{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6715bc2b",
   "metadata": {},
   "source": [
    "# WIP QA Metadata Vector Store\n",
    "\n",
    "Set up a simple Question-Answering system with LangChain and CassIO, using Cassandra as the Vector Database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d9b70",
   "metadata": {},
   "source": [
    "_**NOTE:** this uses Cassandra's \"Vector Similarity Search\" capability.\n",
    "Make sure you are connecting to a vector-enabled database for this demo._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4388ac1d",
   "metadata": {},
   "source": [
    "The following line imports the Cassandra flavor of a LangChain vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.cassandra import Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578a87b",
   "metadata": {},
   "source": [
    "A database connection is needed to access Cassandra. The following assumes\n",
    "that a _vector-search-capable Astra DB instance_ is available. Adjust as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11013224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cqlsession import getCQLSession, getCQLKeyspace\n",
    "cqlMode = 'astra_db' # 'astra_db'/'local'\n",
    "session = getCQLSession(mode=cqlMode)\n",
    "keyspace = getCQLKeyspace(mode=cqlMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2a156",
   "metadata": {},
   "source": [
    "Both an LLM and an embedding function are required.\n",
    "\n",
    "Below is the logic to instantiate the LLM and embeddings of choice. We chose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e3de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llm_choice import suggestLLMProvider\n",
    "\n",
    "llmProvider = suggestLLMProvider()\n",
    "# (Alternatively set llmProvider to 'GCP_VertexAI', 'OpenAI', 'Azure_OpenAI' ... manually if you have credentials)\n",
    "\n",
    "if llmProvider == 'GCP_VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    from langchain.embeddings import VertexAIEmbeddings\n",
    "    llm = VertexAI()\n",
    "    myEmbedding = VertexAIEmbeddings()\n",
    "    print('LLM+embeddings from Vertex AI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    os.environ['OPENAI_API_TYPE'] = 'open_ai'\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = OpenAI(temperature=0)\n",
    "    myEmbedding = OpenAIEmbeddings()\n",
    "    print('LLM+embeddings from OpenAI')\n",
    "elif llmProvider == 'Azure_OpenAI':\n",
    "    os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "    os.environ['OPENAI_API_VERSION'] = os.environ['AZURE_OPENAI_API_VERSION']\n",
    "    os.environ['OPENAI_API_BASE'] = os.environ['AZURE_OPENAI_API_BASE']\n",
    "    os.environ['OPENAI_API_KEY'] = os.environ['AZURE_OPENAI_API_KEY']\n",
    "    from langchain.llms import AzureOpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = AzureOpenAI(temperature=0, model_name=os.environ['AZURE_OPENAI_LLM_MODEL'],\n",
    "                      engine=os.environ['AZURE_OPENAI_LLM_DEPLOYMENT'])\n",
    "    myEmbedding = OpenAIEmbeddings(model=os.environ['AZURE_OPENAI_EMBEDDINGS_MODEL'],\n",
    "                                   deployment=os.environ['AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT'])\n",
    "    print('LLM+embeddings from Azure OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662dd1b-60c0-4c94-9ea1-c23554eda779",
   "metadata": {},
   "source": [
    "## NOTE: skip to \"load from existing\" if loaded already"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f29cf",
   "metadata": {},
   "source": [
    "## A minimal example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf74a31",
   "metadata": {},
   "source": [
    "The following is a minimal usage of the Cassandra vector store. The store is created and filled at once, and is then queried to retrieve relevant parts of the indexed text, which are then stuffed into a prompt finally used to answer a question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29fc57",
   "metadata": {},
   "source": [
    "The following creates an \"index creator\", which knows about the type of vector store, the embedding to use and how to preprocess the input text:\n",
    "\n",
    "_(Note: stores built with different embedding functions will need different tables. This is why we append the `llmProvider` name to the table name in the next cell.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cfe71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'vs_test_md_' + llmProvider\n",
    "\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Cassandra,\n",
    "    embedding=myEmbedding,\n",
    "    text_splitter=CharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=0,\n",
    "    ),\n",
    "    vectorstore_kwargs={\n",
    "        'session': session,\n",
    "        'keyspace': keyspace,\n",
    "        'table_name': table_name,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ddfb6",
   "metadata": {},
   "source": [
    "Loading a local text (a short story by E. A. Poe will do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader1 = TextLoader('texts/amontillado.txt', encoding='utf8')\n",
    "loader2 = TextLoader('texts/mask.txt', encoding='utf8')\n",
    "loader3 = TextLoader('texts/manuscript.txt', encoding='utf8')\n",
    "loaders = [loader1, loader2, loader3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53431eca",
   "metadata": {},
   "source": [
    "This takes a few seconds to run, as it must calculate embedding vectors for a number of chunks of the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4929f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Certain LLM providers need workaround to evaluate batch embeddings\n",
    "#       (as done in next cell).\n",
    "#       As of 2023-06-29, Azure OpenAI would  error with:\n",
    "#           \"InvalidRequestError: Too many inputs. The max number of inputs is 1\"\n",
    "if llmProvider == 'Azure_OpenAI':\n",
    "    from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "    for loader in loaders:\n",
    "        docs = loader.load()\n",
    "        subdocs = index_creator.text_splitter.split_documents(docs)\n",
    "        #\n",
    "        print(f'subdocument {0} ...', end=' ')\n",
    "        vs = index_creator.vectorstore_cls.from_documents(\n",
    "            subdocs[:1],\n",
    "            index_creator.embedding,\n",
    "            **index_creator.vectorstore_kwargs,\n",
    "        )\n",
    "        print('done.')\n",
    "        for sdi, sd in enumerate(subdocs[1:]):\n",
    "            print(f'subdocument {sdi+1} ...', end=' ')\n",
    "            vs.add_texts(texts=[sd.page_content], metadata=[sd.metadata])\n",
    "            print('done.')\n",
    "        #\n",
    "    index = VectorStoreIndexWrapper(vectorstore=vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if llmProvider != 'Azure_OpenAI':\n",
    "    index = index_creator.from_loaders(loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82fdb4-fb51-4623-863c-4a50722ba743",
   "metadata": {},
   "source": [
    "_Note: depending on how you load rows in your store, there might be ways to add your own metadata. Ask Langchain docs! For now, we have a `source` metadata field with the file path, and we'll use that one._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca556b1-3fba-495e-81da-6e07880ffef9",
   "metadata": {},
   "source": [
    "## ... or Load From Existing\n",
    "\n",
    "Use the following cell if the table has been populated already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14bc32-7e54-4438-9ea2-21186e9c4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "myCassandraVStore = Cassandra(\n",
    "    embedding=myEmbedding,\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    table_name='vs_test_md_' + llmProvider,\n",
    ")\n",
    "loaded_index = VectorStoreIndexWrapper(vectorstore=myCassandraVStore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c02057",
   "metadata": {},
   "source": [
    "## QA with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43abfcd2-1f87-4ed0-a6ef-93c52831b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = \"Is the storm scary?\"\n",
    "Q2 = \"Who arrives in the room?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9065b47-77f7-4a8b-a47a-eb65a722fb5d",
   "metadata": {},
   "source": [
    "### No metadata (baseline case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48590e50-f342-422b-87a9-ba6d27a7d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_index.query(Q1))\n",
    "print(\"=\"*20)\n",
    "print(loaded_index.query(Q2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d47074-23fc-4ee1-94c8-7a4e99de7a09",
   "metadata": {},
   "source": [
    "### With metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c65a5d-36a3-4649-91e6-8cebac5dd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_k = {\"search_kwargs\": {\"filter\": {\"source\": \"texts/manuscript.txt\"}}}\n",
    "\n",
    "print(loaded_index.query(Q1, retriever_kwargs=r_k))\n",
    "print(\"=\"*20)\n",
    "print(loaded_index.query(Q2, retriever_kwargs=r_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57970b3a-86e3-4a0d-a140-e34925b93204",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_k2 = {\"search_kwargs\": {\"filter\": {\"source\": \"texts/amontillado.txt\"}}}\n",
    "\n",
    "print(loaded_index.query(Q1, retriever_kwargs=r_k2))\n",
    "print(\"=\"*20)\n",
    "print(loaded_index.query(Q2, retriever_kwargs=r_k2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a4697",
   "metadata": {},
   "source": [
    "## Spawning a \"retriever\" from the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4816cd-88e1-4dbd-ba72-a2bd64522655",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_0 = loaded_index.vectorstore.as_retriever(search_kwargs={\n",
    "    'k': 2,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e816ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_0.get_relevant_documents(Q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c751c0f2-63c1-424f-8c1d-539cb968c94d",
   "metadata": {},
   "source": [
    "### With metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26a655-3f64-4a09-9f51-cf33d0feb568",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_m = loaded_index.vectorstore.as_retriever(search_kwargs={\n",
    "    'k': 2,\n",
    "    'filter': {'source': 'texts/manuscript.txt'},\n",
    "})\n",
    "retriever_m.get_relevant_documents(Q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb97a92-c81c-415d-93ca-a04c77578966",
   "metadata": {},
   "source": [
    "## MMR test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a0dff-8bac-4742-9401-6720b7845b26",
   "metadata": {},
   "source": [
    "### No metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2548e1-0229-40f2-8bdb-91370b9ce108",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qx = \"Who is scared?\"\n",
    "for i, doc in enumerate(myCassandraVStore.search(Qx, search_type='mmr', k=2)):\n",
    "    print(f'[{i:2}]: {doc.metadata[\"source\"]} ==> {doc.page_content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f85f8f-b6b0-495b-bac5-d4b30bf45ed0",
   "metadata": {},
   "source": [
    "### With metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943b097-f795-4ba0-b270-bd50faf6225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(myCassandraVStore.search(Qx, search_type='mmr', k=2, filter={'source': 'texts/amontillado.txt'})):\n",
    "    print(f'[{i:2}]: {doc.metadata[\"source\"]} ==> {doc.page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47663895-4578-43dd-8b1b-f33176e16a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
