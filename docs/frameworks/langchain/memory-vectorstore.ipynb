{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24440ad",
   "metadata": {},
   "source": [
    "# VectorStore-backed memory\n",
    "\n",
    "The support for Cassandra vector store, available in LangChain, enables another interesting use case, namely a chat memory buffer that injects the most relevant past exchanges into the prompt, instead of the most recent (as most other memories do). This enables retrieval of related context _arbitrarily far back in the chat history_.\n",
    "\n",
    "All you need is to instantiate a `Cassandra` vector store and wrap it in a `VectorStoreRetrieverMemory` type of memory, provided by LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d089d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5eca9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.cassandra import Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d19647",
   "metadata": {},
   "source": [
    "As usual, a database connection is needed to access Cassandra. The following assumes\n",
    "that a _vector-search-capable Astra DB instance_ is available. Adjust as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf9f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cqlsession import getCQLSession, getCQLKeyspace\n",
    "cqlMode = 'astra_db' # 'astra_db'/'local'\n",
    "session = getCQLSession(mode=cqlMode)\n",
    "keyspace = getCQLKeyspace(mode=cqlMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b5aae8",
   "metadata": {},
   "source": [
    "Both an LLM and an embedding function are required.\n",
    "\n",
    "Below is the logic to instantiate the LLM and embeddings of choice. We choose to leave it in the notebooks for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14603440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM+embeddings from VertexAI\n"
     ]
    }
   ],
   "source": [
    "from llm_choice import suggestLLMProvider\n",
    "\n",
    "llmProvider = 'VertexAI' # suggestLLMProvider()\n",
    "# (Alternatively set llmProvider to 'VertexAI', 'OpenAI' ... manually if you have credentials)\n",
    "\n",
    "if llmProvider == 'VertexAI':\n",
    "    from langchain.llms import VertexAI\n",
    "    from langchain.embeddings import VertexAIEmbeddings\n",
    "    llm = VertexAI()\n",
    "    myEmbedding = VertexAIEmbeddings()\n",
    "    print('LLM+embeddings from VertexAI')\n",
    "elif llmProvider == 'OpenAI':\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    llm = OpenAI(temperature=0)\n",
    "    myEmbedding = OpenAIEmbeddings()\n",
    "    print('LLM+embeddings from OpenAI')\n",
    "else:\n",
    "    raise ValueError('Unknown LLM provider.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8393b",
   "metadata": {},
   "source": [
    "## Create the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b2b44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'vstore_memory_' + llmProvider\n",
    "cassVStore = Cassandra(\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    table_name=table_name,\n",
    "    embedding=myEmbedding,\n",
    ")\n",
    "\n",
    "# just in case this demo runs multiple times\n",
    "cassVStore.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2843410",
   "metadata": {},
   "source": [
    "## Create the retriever and the memory\n",
    "\n",
    "From the vector store a \"retriever\" is spawned. You'll keep the number of items to fetch intentionally very small for demonstration purposes.\n",
    "\n",
    "Next, the retriever is wrapped in a `VectorStoreRetrieverMemory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62bdb85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = cassVStore.as_retriever(search_kwargs={'k': 3})\n",
    "semanticMemory = VectorStoreRetrieverMemory(retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a4cd9a",
   "metadata": {},
   "source": [
    "Create a fake \"past conversation\". Note how the topic of the discussion wanders to fixing one's PC in the last few exchanges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e723384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pastExchanges = [\n",
    "    (\n",
    "        {\"input\": \"Hello, what is the biggest mammal?\"},\n",
    "        {\"output\": \"The blue whale.\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"input\": \"... I cannot swim. Actually I hate swimming!\"},\n",
    "        {\"output\": \"I see.\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"input\": \"I like mountains and beech forests.\"},\n",
    "        {\"output\": \"That's good to know.\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"input\": \"Yes, too much water makes me uneasy.\"},\n",
    "        {\"output\": \"Ah, how come?.\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"input\": \"I guess I am just not a seaside person\"},\n",
    "        {\"output\": \"I see. How may I help you?\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"input\": \"I need help installing this driver\"},\n",
    "        {\"output\": \"First download the right version for your operating system.\"},\n",
    "    ),\n",
    "    (\n",
    "        {\"input\": \"Good grief ... my keyboard does not work anymore!\"},\n",
    "        {\"output\": \"Try plugging it in your PC first.\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b9f4b",
   "metadata": {},
   "source": [
    "Insert these exchanges into the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c114f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exI, exO in pastExchanges:\n",
    "    semanticMemory.save_context(exI, exO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4c1c59",
   "metadata": {},
   "source": [
    "Given a conversation input, the `load_memory_variables` performs a semantic search and comes up with relevant items from the memory, regardless of their order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e54892",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Can you suggest me a sport to try?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a646c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: I guess I am just not a seaside person\n",
      "output: I see. How may I help you?\n",
      "input: ... I cannot swim. Actually I hate swimming!\n",
      "output: I see.\n",
      "input: I like mountains and beech forests.\n",
      "output: That's good to know.\n"
     ]
    }
   ],
   "source": [
    "print(semanticMemory.load_memory_variables({\"prompt\": QUESTION})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3689b2",
   "metadata": {},
   "source": [
    "## Usage in a conversation chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95178b81",
   "metadata": {},
   "source": [
    "This semantic memory element can be used within a full conversation chain.\n",
    "\n",
    "In the following you'll create a custom prompt and a `ConversationChain` out of it, attaching the latter to the vector-store-powered memory seen above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a946b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "semanticMemoryTemplateString = \"\"\"The following is a between a human and a helpful AI.\n",
    "The AI is talkative and provides lots of specific details from its context.\n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "The AI can use information from parts of the previous conversation (only if they are relevant):\n",
    "{history}\n",
    "\n",
    "Current conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "memoryPrompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=semanticMemoryTemplateString\n",
    ")\n",
    "\n",
    "conversationWithVectorRetrieval = ConversationChain(\n",
    "    llm=llm, \n",
    "    prompt=memoryPrompt,\n",
    "    memory=semanticMemory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e6d4a",
   "metadata": {},
   "source": [
    "Run the chain with the sports question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ff9fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a between a human and a helpful AI.\n",
      "The AI is talkative and provides lots of specific details from its context.\n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "The AI can use information from parts of the previous conversation (only if they are relevant):\n",
      "input: I guess I am just not a seaside person\n",
      "output: I see. How may I help you?\n",
      "input: ... I cannot swim. Actually I hate swimming!\n",
      "output: I see.\n",
      "input: I like mountains and beech forests.\n",
      "output: That's good to know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Can you suggest me a sport to try?\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "400 40.000000 is out of supported range [0, 1];  for top_p.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:72\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/grpc/_channel.py:1030\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1028\u001b[0m state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[1;32m   1029\u001b[0m                               wait_for_ready, compression)\n\u001b[0;32m-> 1030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/grpc/_channel.py:910\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"40.000000 is out of supported range [0, 1];  for top_p.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.180.170:443 {grpc_message:\"40.000000 is out of supported range [0, 1];  for top_p.\", grpc_status:3, created_time:\"2023-06-06T16:44:01.835672084+02:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconversationWithVectorRetrieval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQUESTION\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal/Datastax/code/my_github/langchain/langchain/chains/llm.py:213\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/personal/Datastax/code/my_github/langchain/langchain/chains/base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/personal/Datastax/code/my_github/langchain/langchain/chains/base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    128\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    129\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    130\u001b[0m     inputs,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 134\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/personal/Datastax/code/my_github/langchain/langchain/chains/llm.py:69\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     66\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m     67\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     68\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/personal/Datastax/code/my_github/langchain/langchain/chains/llm.py:79\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal/Datastax/code/my_github/langchain/langchain/llms/base.py:134\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    129\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m    130\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    131\u001b[0m     callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    132\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    133\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal/Datastax/code/my_github/langchain/langchain/llms/base.py:191\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    192\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(output)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/personal/Datastax/code/my_github/langchain/langchain/llms/base.py:185\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    180\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    181\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m}, prompts, invocation_params\u001b[38;5;241m=\u001b[39mparams\n\u001b[1;32m    182\u001b[0m )\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 185\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/personal/Datastax/code/my_github/langchain/langchain/llms/base.py:436\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager)\u001b[0m\n\u001b[1;32m    433\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m    435\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 436\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    439\u001b[0m     )\n\u001b[1;32m    440\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/personal/Datastax/code/my_github/langchain/langchain/llms/vertexai.py:110\u001b[0m, in \u001b[0;36mVertexAI._call\u001b[0;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     96\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     97\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     98\u001b[0m     run_manager: Optional[CallbackManagerForLLMRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call Vertex model to get predictions based on the prompt.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        The string generated by the model.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/personal/Datastax/code/my_github/langchain/langchain/llms/vertexai.py:52\u001b[0m, in \u001b[0;36m_VertexAICommon._predict\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enforce_stop_words(res\u001b[38;5;241m.\u001b[39mtext, stop)\n",
      "File \u001b[0;32m~/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/vertexai/language_models/_language_models.py:327\u001b[0m, in \u001b[0;36mTextGenerationModel.predict\u001b[0;34m(self, prompt, max_output_tokens, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    307\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m     top_p: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m _DEFAULT_TOP_P,\n\u001b[1;32m    313\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextGenerationResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets model response for a single prompt.\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m        A `TextGenerationResponse` object that contains the text produced by the model.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/vertexai/language_models/_language_models.py:363\u001b[0m, in \u001b[0;36mTextGenerationModel._batch_predict\u001b[0;34m(self, prompts, max_output_tokens, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m    355\u001b[0m instances \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(prompt)} \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m    356\u001b[0m prediction_parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxDecodeSteps\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_output_tokens,\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopP\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopK\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_k,\n\u001b[1;32m    361\u001b[0m }\n\u001b[0;32m--> 363\u001b[0m prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    369\u001b[0m     TextGenerationResponse(\n\u001b[1;32m    370\u001b[0m         text\u001b[38;5;241m=\u001b[39mprediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m prediction_response\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[1;32m    374\u001b[0m ]\n",
      "File \u001b[0;32m~/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:1546\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1534\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mjson_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1535\u001b[0m         deployed_model_id\u001b[38;5;241m=\u001b[39mraw_predict_response\u001b[38;5;241m.\u001b[39mheaders[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1543\u001b[0m         ),\n\u001b[1;32m   1544\u001b[0m     )\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1546\u001b[0m     prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1554\u001b[0m         predictions\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   1555\u001b[0m             json_format\u001b[38;5;241m.\u001b[39mMessageToDict(item)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1560\u001b[0m         model_resource_name\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m   1561\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:602\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    597\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m    598\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mendpoint),)),\n\u001b[1;32m    599\u001b[0m )\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[1;32m    111\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/langchain-cassio-3.10/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:74\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 40.000000 is out of supported range [0, 1];  for top_p."
     ]
    }
   ],
   "source": [
    "conversationWithVectorRetrieval.predict(input=QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20059880",
   "metadata": {},
   "source": [
    "Notice how new exchanges are automatically added to the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356fd41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversationWithVectorRetrieval.predict(input=\"Would I like a swim in a mountain lake?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4bfad9",
   "metadata": {},
   "source": [
    "... so that now the most relevant items for the same question are changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "semanticMemory.retriever.get_relevant_documents(QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa99314",
   "metadata": {},
   "source": [
    "## A counterexample\n",
    "\n",
    "What would happen with a simpler memory element, which simply retrieves a certain number of most recent interactions?\n",
    "\n",
    "Create and populate an instance of LangChain's `ConversationTokenBufferMemory`, limiting it to a maximum token length of 80 (roughly equivalent to the 3 fragments set for the `semanticMemory` object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4306aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31863f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseHistory = ChatMessageHistory()\n",
    "\n",
    "recencyBufferMemory = ConversationTokenBufferMemory(\n",
    "    chat_memory=baseHistory,\n",
    "    max_token_limit=80,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exI, exO in pastExchanges:\n",
    "    recencyBufferMemory.save_context(exI, exO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7eb975",
   "metadata": {},
   "source": [
    "Time to ask the same sports question. This is what will get injected into the prompt this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recencyBufferMemory.load_memory_variables({\"prompt\": QUESTION})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18863525",
   "metadata": {},
   "source": [
    "... and this is the (rather generic) answer you'd get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc52f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversationWithRecencyRetrieval = ConversationChain(\n",
    "    llm=llm, \n",
    "    prompt=memoryPrompt,\n",
    "    memory=recencyBufferMemory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversationWithRecencyRetrieval.predict(input=QUESTION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
